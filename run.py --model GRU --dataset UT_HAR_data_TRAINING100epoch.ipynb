{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1Cz2jHsRGlIY8OLCF5RPplMv55tD0VoXd",
      "authorship_tag": "ABX9TyNQWZZ2ciUBe37aJC3dPkfv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuo-liangliang/fcn.berkeleyvision.org/blob/master/run.py%20--model%20GRU%20--dataset%20UT_HAR_data_TRAINING100epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oS3ZliHYxQ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/UT_HAR.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/UT_HAR/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "XOjCBS_TDq4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "7hxwHXeDxR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koT1oiq_vVPj"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 删除指定的含有userx的数据\n",
        "################################################################################################################################\n",
        "import os\n",
        "\n",
        "# 定义目标文件夹路径\n",
        "# 1-3\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/1-Push&Pull'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/2-Sweep'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/3-Clap'\n",
        "\n",
        "# 4\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/4-Slide'\n",
        "\n",
        "# 5\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/5-Draw-N(H)'\n",
        "\n",
        "\n",
        "# 6-9\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/6-Draw-O(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/7-Draw-Rectangle(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/8-Draw-Triangle(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/9-Draw-Zigzag(H)'\n",
        "\n",
        "\n",
        "# 10-12\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/10-Draw-Zigzag(V)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/11-Draw-N(V)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/12-Draw-O(V)'\n",
        "\n",
        "\n",
        "# 13-22\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/13-Draw-1'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/14-Draw-2'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/15-Draw-3'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/16-Draw-4'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/17-Draw-5'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/18-Draw-6'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/19-Draw-7'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/20-Draw-8'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/21-Draw-9'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/22-Draw-10'\n",
        "\n",
        "# 获取目标文件夹中所有文件名\n",
        "files_before = os.listdir(folder_path)\n",
        "\n",
        "for file_name in files_before:\n",
        "    # 1-3\n",
        "    # 删除以'user3'到'user5'和'user8'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user4') or file_name.startswith('user5') or file_name.startswith('user8') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 4\n",
        "    # 删除以'user3'和'user5'和'user8'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user5') or file_name.startswith('user8') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 5\n",
        "    # 删除以'user3'和'user5'和'user9'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user5') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 6-9\n",
        "    # 删除以'user3'和'user9'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "\n",
        "    # 10-12\n",
        "    # 删除以'user2'到'user4'开头的文件\n",
        "    # if file_name.startswith('user2') or file_name.startswith('user3') or file_name.startswith('user4') :\n",
        "\n",
        "\n",
        "    # 13-22\n",
        "    # 删除以'user2'开头的文件\n",
        "    # if file_name.startswith('user2') :\n",
        "        os.remove(os.path.join(folder_path, file_name))\n",
        "\n",
        "\n",
        "# 再次获取目标文件夹中所有文件名\n",
        "files_after = os.listdir(folder_path)\n",
        "\n",
        "# 输出删除前后的文件数量\n",
        "print(\"删除前的文件数量:\", len(files_before))\n",
        "print(\"删除后的文件数量:\", len(files_after))\n",
        "\n"
      ],
      "metadata": {
        "id": "xiRzs_rbxvkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amX2d-r66deM",
        "outputId": "3938ae77-a187-475a-e8af-cfe1cc5461b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "##############验证程序############验证程序###########验证程序##############验证程序########验证程序###########验证程序###########\n",
        "################################################################################################################################\n",
        "!pip install einops\n",
        "# !pip install torch==1.12.0 torchvision==0.13.0\n",
        "import torch\n",
        "import torchvision\n",
        "#!pip install torch torchvision torchaudio\n",
        "print(\"PyTorch 版本:\", torch.__version__)\n",
        "print(\"TorchVision 版本:\", torchvision.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV7neAy05cTe",
        "outputId": "7bad8452-f373-4099-95a8-8e6c4467a1ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "PyTorch 版本: 2.2.1+cu121\n",
            "TorchVision 版本: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main\"\n",
        "os.chdir(path)\n",
        "print(os.getcwd())\n",
        "# 更改运行目录，定位当前运行地址"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMHKhhQO5jgI",
        "outputId": "186721c5-c99a-43bd-9ce5-4b6cbeff39b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python run.py --model CNN+GRU --dataset Widar\n",
        "# !python run.py --model CNN+GRU --dataset UT_HAR_data\n",
        "!python run.py --model GRU --dataset UT_HAR_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vQk3ucx5lUN",
        "outputId": "36185eda-919b-495e-de99-4493f9c2a526"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using dataset: UT-HAR DATA\n",
            "using model: GRU\n",
            "Epoch:1, Accuracy:0.3107,Loss:1.783632451\n",
            "Epoch:2, Accuracy:0.3415,Loss:1.713753040\n",
            "Epoch:3, Accuracy:0.3788,Loss:1.629627482\n",
            "Epoch:4, Accuracy:0.4390,Loss:1.500593055\n",
            "Epoch:5, Accuracy:0.4814,Loss:1.436277898\n",
            "Epoch:6, Accuracy:0.4924,Loss:1.394281636\n",
            "Epoch:7, Accuracy:0.5129,Loss:1.336968976\n",
            "Epoch:8, Accuracy:0.5449,Loss:1.259437738\n",
            "Epoch:9, Accuracy:0.5718,Loss:1.204207699\n",
            "Epoch:10, Accuracy:0.5756,Loss:1.164806272\n",
            "Epoch:11, Accuracy:0.5854,Loss:1.129252259\n",
            "Epoch:12, Accuracy:0.5910,Loss:1.103444380\n",
            "Epoch:13, Accuracy:0.6215,Loss:1.038774501\n",
            "Epoch:14, Accuracy:0.6134,Loss:1.039108131\n",
            "Epoch:15, Accuracy:0.6436,Loss:0.968557856\n",
            "Epoch:16, Accuracy:0.6671,Loss:0.924072094\n",
            "Epoch:17, Accuracy:0.6648,Loss:0.916110651\n",
            "Epoch:18, Accuracy:0.6656,Loss:0.903388965\n",
            "Epoch:19, Accuracy:0.6812,Loss:0.881016183\n",
            "Epoch:20, Accuracy:0.6878,Loss:0.845225983\n",
            "Epoch:21, Accuracy:0.6822,Loss:0.859492624\n",
            "Epoch:22, Accuracy:0.7089,Loss:0.796018840\n",
            "Epoch:23, Accuracy:0.7036,Loss:0.769188796\n",
            "Epoch:24, Accuracy:0.7135,Loss:0.783320604\n",
            "Epoch:25, Accuracy:0.7233,Loss:0.737947651\n",
            "Epoch:26, Accuracy:0.6958,Loss:0.812489509\n",
            "Epoch:27, Accuracy:0.7225,Loss:0.745991834\n",
            "Epoch:28, Accuracy:0.7460,Loss:0.681540178\n",
            "Epoch:29, Accuracy:0.7319,Loss:0.707701458\n",
            "Epoch:30, Accuracy:0.7412,Loss:0.671893940\n",
            "Epoch:31, Accuracy:0.7588,Loss:0.644218562\n",
            "Epoch:32, Accuracy:0.7518,Loss:0.647044077\n",
            "Epoch:33, Accuracy:0.7487,Loss:0.666333538\n",
            "Epoch:34, Accuracy:0.7538,Loss:0.646243650\n",
            "Epoch:35, Accuracy:0.7765,Loss:0.589587120\n",
            "Epoch:36, Accuracy:0.7634,Loss:0.617539285\n",
            "Epoch:37, Accuracy:0.7724,Loss:0.590769952\n",
            "Epoch:38, Accuracy:0.7850,Loss:0.557631774\n",
            "Epoch:39, Accuracy:0.6300,Loss:1.087757433\n",
            "Epoch:40, Accuracy:0.4929,Loss:1.395160022\n",
            "Epoch:41, Accuracy:0.5691,Loss:1.179489742\n",
            "Epoch:42, Accuracy:0.6326,Loss:1.020379708\n",
            "Epoch:43, Accuracy:0.6696,Loss:0.934015818\n",
            "Epoch:44, Accuracy:0.6915,Loss:0.863735072\n",
            "Epoch:45, Accuracy:0.7102,Loss:0.819583194\n",
            "Epoch:46, Accuracy:0.7326,Loss:0.762436944\n",
            "Epoch:47, Accuracy:0.7145,Loss:0.803426050\n",
            "Epoch:48, Accuracy:0.7573,Loss:0.695316628\n",
            "Epoch:49, Accuracy:0.7213,Loss:0.785674535\n",
            "Epoch:50, Accuracy:0.7623,Loss:0.669507407\n",
            "Epoch:51, Accuracy:0.7525,Loss:0.672632741\n",
            "Epoch:52, Accuracy:0.7797,Loss:0.618116382\n",
            "Epoch:53, Accuracy:0.7853,Loss:0.608745906\n",
            "Epoch:54, Accuracy:0.7684,Loss:0.662241783\n",
            "Epoch:55, Accuracy:0.8075,Loss:0.558463185\n",
            "Epoch:56, Accuracy:0.7979,Loss:0.568282942\n",
            "Epoch:57, Accuracy:0.7951,Loss:0.572009424\n",
            "Epoch:58, Accuracy:0.7593,Loss:0.654124909\n",
            "Epoch:59, Accuracy:0.8102,Loss:0.535629257\n",
            "Epoch:60, Accuracy:0.8289,Loss:0.485528827\n",
            "Epoch:61, Accuracy:0.8241,Loss:0.497804880\n",
            "Epoch:62, Accuracy:0.8427,Loss:0.460051686\n",
            "Epoch:63, Accuracy:0.8246,Loss:0.510229400\n",
            "Epoch:64, Accuracy:0.8233,Loss:0.499109826\n",
            "Epoch:65, Accuracy:0.8304,Loss:0.477802389\n",
            "Epoch:66, Accuracy:0.6242,Loss:1.151274061\n",
            "Epoch:67, Accuracy:0.7918,Loss:0.594351503\n",
            "Epoch:68, Accuracy:0.8196,Loss:0.504139698\n",
            "Epoch:69, Accuracy:0.8264,Loss:0.488454253\n",
            "Epoch:70, Accuracy:0.8395,Loss:0.469102149\n",
            "Epoch:71, Accuracy:0.8264,Loss:0.484350305\n",
            "Epoch:72, Accuracy:0.8632,Loss:0.417719889\n",
            "Epoch:73, Accuracy:0.8642,Loss:0.387003842\n",
            "Epoch:74, Accuracy:0.8523,Loss:0.428504006\n",
            "Epoch:75, Accuracy:0.8611,Loss:0.404738005\n",
            "Epoch:76, Accuracy:0.8851,Loss:0.349531153\n",
            "Epoch:77, Accuracy:0.8569,Loss:0.423009038\n",
            "Epoch:78, Accuracy:0.8816,Loss:0.355238255\n",
            "Epoch:79, Accuracy:0.8382,Loss:0.463470918\n",
            "Epoch:80, Accuracy:0.8634,Loss:0.394007641\n",
            "Epoch:81, Accuracy:0.8707,Loss:0.368090767\n",
            "Epoch:82, Accuracy:0.8863,Loss:0.334750058\n",
            "Epoch:83, Accuracy:0.8889,Loss:0.334972535\n",
            "Epoch:84, Accuracy:0.8057,Loss:0.580265381\n",
            "Epoch:85, Accuracy:0.8705,Loss:0.376100435\n",
            "Epoch:86, Accuracy:0.8911,Loss:0.339251739\n",
            "Epoch:87, Accuracy:0.8705,Loss:0.378665162\n",
            "Epoch:88, Accuracy:0.9007,Loss:0.294869331\n",
            "Epoch:89, Accuracy:0.9007,Loss:0.291913168\n",
            "Epoch:90, Accuracy:0.8919,Loss:0.308492172\n",
            "Epoch:91, Accuracy:0.8780,Loss:0.354803925\n",
            "Epoch:92, Accuracy:0.8808,Loss:0.333395747\n",
            "Epoch:93, Accuracy:0.8803,Loss:0.331782039\n",
            "Epoch:94, Accuracy:0.8768,Loss:0.347748595\n",
            "Epoch:95, Accuracy:0.8808,Loss:0.354097779\n",
            "Epoch:96, Accuracy:0.9002,Loss:0.281607935\n",
            "Epoch:97, Accuracy:0.9070,Loss:0.275137226\n",
            "Epoch:98, Accuracy:0.8753,Loss:0.368458862\n",
            "Epoch:99, Accuracy:0.9060,Loss:0.274358271\n",
            "Epoch:100, Accuracy:0.9254,Loss:0.231246720\n",
            "validation accuracy:0.8730, loss:0.34312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定位到指定路径下\n",
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/SeptemberResult/result_DNN_CCE_epoch250')\n",
        "os.chdir('/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng')\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls5ZDn3v2AAV",
        "outputId": "d2e7157b-6038-48f3-de40-2062797d4250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEDRT-LSTM\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# time    : 2023/9/15 00:32\n",
        "# Author  : Liang-liang Zuo\n",
        "# @FileName: FEDRT-LSTM.py  DeepLearningBK-ZuoLiangliang/result_MultiAttention_LSTM_1层Transformer_MSE_epoch200_batchsize32_lr0.001_test0.1_95.54%.ipynb\n",
        "# @Software: Colab_Notebooks\n",
        "# 链接地址：https://github.com/zuo-liangliang/DeepLearningBK-ZuoLiangliang/blob/main/result_MultiAttention_LSTM_1%E5%B1%82Transformer_MSE_epoch200_batchsize32_lr0.001_test0.1_95.54%25.ipynb\n",
        "\n",
        "# 下载文件夹  计划下载的文件夹压成压缩包\n",
        "# import shutil\n",
        "\n",
        "# source_folder = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_Transformer_MultiAttention_LSTM_改costfunction为mse_1层Transformer_epoch200'  # 要压缩的文件夹路径\n",
        "# destination_zip = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/zip/result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_Transformer_MultiAttention_LSTM_改costfunction为mse_1层Transformer_epoch200'  # 压缩包保存路径\n",
        "\n",
        "# shutil.make_archive(destination_zip, 'zip', source_folder)\n",
        "\n",
        "# 定位到指定路径下\n",
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/SeptemberResult/result_MSE_epoch200_lr0.001_batchsize32所有网络汇总')\n",
        "# print(os.getcwd())\n",
        "\n",
        "# /content/drive/MyDrive/SeptemberResult/result_MSE_epoch200_lr0.001_batchsize32所有网络汇总\n",
        "\n",
        "# result_MultiAttention_LSTM_1层Transformer\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as scio\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.keras.backend.set_session\n",
        "import keras\n",
        "from keras.layers import Input, GRU, LSTM, Dense, Flatten, Dropout, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, TimeDistributed\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization  #加Transformer修改点1\n",
        "num_transformer_layers = 1  # 设置Transformer层数 #加Transformer修改点2  2层-->1层\n",
        "\n",
        "# Parameters\n",
        "# use_existing_model = False  # 创建一个模型（False）默认创建一个新模型\n",
        "use_existing_model = True    # 模型权重修改点 使用已经存在的模型（True）\n",
        "# data_dir = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/'   #修改数据集文件夹\n",
        "\n",
        "data_dir_training = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/'\n",
        "data_dir_testing = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/'  # 修改为另外环境下的数据路径\n",
        "\n",
        "#fraction_for_test = 0.1 # 修改测试集占比 原先是0.1-->0.2\n",
        "#ALL_MOTION = [1,2,3,4,5,6]\n",
        "ALL_MOTION = [2,3,4,5]\n",
        "N_MOTION = len(ALL_MOTION)\n",
        "T_MAX = 0\n",
        "n_epochs = 100       #原始是30 #修改点1 取在528上最好的一组超参\n",
        "f_dropout_ratio = 0.5\n",
        "n_gru_hidden_units = 32\n",
        "n_batch_size = 32     #原始是32-->16\n",
        "f_learning_rate = 0.0001  #原始是0.001-->0.002\n",
        "\n",
        "\n",
        "def normalize_data(data_1):\n",
        "    # data(ndarray)=>data_norm(ndarray): [20,20,T]=>[20,20,T]\n",
        "    data_1_max = np.concatenate((data_1.max(axis=0),data_1.max(axis=1)),axis=0).max(axis=0)\n",
        "    data_1_min = np.concatenate((data_1.min(axis=0),data_1.min(axis=1)),axis=0).min(axis=0)\n",
        "    if (len(np.where((data_1_max - data_1_min) == 0)[0]) > 0):\n",
        "        return data_1\n",
        "    data_1_max_rep = np.tile(data_1_max,(data_1.shape[0],data_1.shape[1],1))\n",
        "    data_1_min_rep = np.tile(data_1_min,(data_1.shape[0],data_1.shape[1],1))\n",
        "    data_1_norm = (data_1 - data_1_min_rep) / (data_1_max_rep - data_1_min_rep)\n",
        "    return  data_1_norm\n",
        "\n",
        "def zero_padding(data, T_MAX):\n",
        "    # data(list)=>data_pad(ndarray): [20,20,T1/T2/...]=>[20,20,T_MAX]\n",
        "    data_pad = []\n",
        "    for i in range(len(data)):\n",
        "        t = np.array(data[i]).shape[2]\n",
        "        data_pad.append(np.pad(data[i], ((0,0),(0,0),(T_MAX - t,0)), 'constant', constant_values = 0).tolist())\n",
        "    return np.array(data_pad)\n",
        "\n",
        "def onehot_encoding(label, num_class):\n",
        "    # label(list)=>_label(ndarray): [N,]=>[N,num_class]\n",
        "    label = np.array(label).astype('int32')\n",
        "    # assert (np.arange(0,np.unique(label).size)==np.unique(label)).prod()    # Check label from 0 to N\n",
        "    label = np.squeeze(label)\n",
        "    # _label = np.eye(num_class)[label-1]     # from label to onehot\n",
        "    _label = np.eye(num_class)[label-2]   #修改点2\n",
        "    return _label\n",
        "\n",
        "def load_data(path_to_data, motion_sel):\n",
        "    global T_MAX\n",
        "    data = []\n",
        "    label = []\n",
        "    for data_root, data_dirs, data_files in os.walk(path_to_data):\n",
        "        for data_file_name in data_files:\n",
        "\n",
        "            file_path = os.path.join(data_root,data_file_name)\n",
        "            try:\n",
        "                data_1 = scio.loadmat(file_path)['velocity_spectrum_ro']\n",
        "                label_1 = int(data_file_name.split('-')[1])\n",
        "                #location = int(data_file_name.split('-')[2])\n",
        "                #orientation = int(data_file_name.split('-')[3])\n",
        "                #repetition = int(data_file_name.split('-')[4])\n",
        "                repetition = int(data_file_name.split('-')[2])  #修改点3\n",
        "\n",
        "                # Select Motion\n",
        "                if (label_1 not in motion_sel):\n",
        "                    continue\n",
        "\n",
        "                # Select Location\n",
        "                # if (location not in [1,2,3,5]):\n",
        "                #     continue\n",
        "\n",
        "                # Select Orientation\n",
        "                # if (orientation not in [1,2,4,5]):\n",
        "                #     continue\n",
        "\n",
        "                # Normalization\n",
        "                data_normed_1 = normalize_data(data_1)\n",
        "\n",
        "                # Update T_MAX\n",
        "                if T_MAX < np.array(data_1).shape[2]:\n",
        "                    T_MAX = np.array(data_1).shape[2]\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # Save List\n",
        "            data.append(data_normed_1.tolist())\n",
        "            label.append(label_1)\n",
        "                # Zero-padding\n",
        "    data = zero_padding(data, T_MAX)\n",
        "\n",
        "    # Swap axes\n",
        "    data = np.swapaxes(np.swapaxes(data, 1, 3), 2, 3)   # [N,20,20',T_MAX]=>[N,T_MAX,20,20']\n",
        "    data = np.expand_dims(data, axis=-1)    # [N,T_MAX,20,20]=>[N,T_MAX,20,20,1]\n",
        "\n",
        "    # Convert label to ndarray\n",
        "    label = np.array(label)\n",
        "\n",
        "    # data(ndarray): [N,T_MAX,20,20,1], label(ndarray): [N,N_MOTION]\n",
        "    return data, label\n",
        "\n",
        "# 构建符合输入形状为 (None, 20, 15, 15, 1) 和输出形状为 (None, 4) 的  模型：\n",
        "\n",
        "#加Transformer修改点3\n",
        "def transformer_layer(inputs, hidden_units, dropout_rate):\n",
        "    # Self-attention\n",
        "    attention_output = MultiHeadAttention(\n",
        "        num_heads=8, key_dim=hidden_units // 8\n",
        "    )(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = LayerNormalization()(attention_output + inputs)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(hidden_units, activation=\"relu\")(attention_output)\n",
        "    ffn = Dense(hidden_units)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "    ffn = LayerNormalization()(ffn)\n",
        "\n",
        "    return ffn\n",
        "\n",
        "#加Transformer修改点4\n",
        "def assemble_model(input_shape, n_class):\n",
        "    model_input = Input(shape=input_shape, dtype='float32', name='name_model_input')    # (@,T_MAX,20,20,1)\n",
        "\n",
        "    # Feature extraction part\n",
        "    x = TimeDistributed(Conv2D(16,kernel_size=(5,5),activation='relu',data_format='channels_last',\\\n",
        "        input_shape=input_shape))(model_input)   # (@,T_MAX,20,20,1)=>(@,T_MAX,16,16,16)\n",
        "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)    # (@,T_MAX,16,16,16)=>(@,T_MAX,8,8,16)\n",
        "    x = TimeDistributed(Flatten())(x)   # (@,T_MAX,8,8,16)=>(@,T_MAX,8*8*16)\n",
        "    x = TimeDistributed(Dense(64,activation='relu'))(x) # (@,T_MAX,8*8*16)=>(@,T_MAX,64)\n",
        "    x = TimeDistributed(Dropout(f_dropout_ratio))(x)\n",
        "    x = TimeDistributed(Dense(64,activation='relu'))(x) # (@,T_MAX,64)=>(@,T_MAX,64)\n",
        "\n",
        "    # Add transformer layers\n",
        "    for _ in range(num_transformer_layers):\n",
        "        #x = transformer_layer(x, hidden_units=n_gru_hidden_units, dropout_rate=f_dropout_ratio)\n",
        "        x = transformer_layer(x, hidden_units=64, dropout_rate=f_dropout_ratio)  # 修改点1\n",
        "    x = LSTM(n_gru_hidden_units,return_sequences=False)(x)  # (@,T_MAX,64)=>(@,128)\n",
        "    x = Dropout(f_dropout_ratio)(x)\n",
        "    model_output = Dense(n_class, activation='softmax', name='name_model_output')(x)  # (@,128)=>(@,n_class)\n",
        "\n",
        "    # Create the model\n",
        "    # Model compiling\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(lr=f_learning_rate),\n",
        "                    # loss='categorical_crossentropy', # 将损失函数改为多类别交叉熵损失函数\n",
        "                    loss='mean_squared_error',  # 将损失函数改为均方误差\n",
        "                    # loss='mean_absolute_error',  # 将损失函数改为平均绝对误差\n",
        "                    # loss='binary_crossentropy',  # 将损失函数改为对数损失函数\n",
        "\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "    return model\n",
        "\n",
        "# 改\n",
        "#model_type = '双层LSTM'\n",
        "#model_type = 'LSTM'\n",
        "#model_type = 'BiLSTM'\n",
        "#model_type = 'MultiAttention_LSTM_1层Transformer'\n",
        "model_type = 'FEDRT-LSTM'\n",
        "\n",
        "# 创建结果文件夹\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_GRU_改costfunction为mse_epoch200' # 每次都要改\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_categorical_crossentropy_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_mean_squared_error_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_mean_absolute_error_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_binary_crossentropy_epoch{n_epochs}'\n",
        "\n",
        "# result_folder = f'result_{model_type}_CCE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "# result_folder = f'result_{model_type}_MSE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "result_folder = f'Result_528TRAIN-zhangmengTEST'\n",
        "\n",
        "# result_folder = f'result_{model_type}_MAE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}'\n",
        "# result_folder = f'result_{model_type}_BCE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "\n",
        "os.makedirs(result_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "RcrUiRWM2gKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's BEGIN >>>>\n",
        "\n",
        "import time\n",
        "start_time = time.time()  # 记录程序开始时间\n",
        "\n",
        "# 获取可见的GPU设备列表\n",
        "# visible_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# 检查是否存在可见的GPU设备\n",
        "# if len(visible_gpus) < 1:\n",
        "    # print('No GPU available')\n",
        "    # exit(0)\n",
        "# 设置使用第一个可见的GPU设备\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(visible_gpus[0].index)\n",
        "# 配置GPU选项\n",
        "# gpu_config = tf.config.experimental.set_memory_growth(visible_gpus[0], True)\n",
        "# 创建一个会话并应用GPU配置\n",
        "# sess = tf.compat.v1.Session(config=gpu_config)\n",
        "# tf.compat.v1.keras.backend.set_session(sess)\n",
        "# tf.random.set_seed(1)\n",
        "\n",
        "# 获取可见的 GPU 设备列表\n",
        "visible_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(visible_gpus) > 0:\n",
        "    # 如果有可见的 GPU 设备，设置 TensorFlow 在 GPU 上运行\n",
        "    tf.config.set_visible_devices(visible_gpus, 'GPU')\n",
        "    gpu_config = tf.config.experimental.set_memory_growth(visible_gpus[0], True)\n",
        "    sess = tf.compat.v1.Session(config=gpu_config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "    print('GPU available')\n",
        "else:\n",
        "    # 如果没有可见的 GPU 设备，设置 TensorFlow 在 CPU 上运行\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "    sess = tf.compat.v1.Session()\n",
        "    print('No GPU available')\n",
        "\n",
        "# 加载训练集数据\n",
        "train_data, train_label = load_data(data_dir_training, ALL_MOTION)\n",
        "print('\\nLoaded dataset of ' + str(train_label.shape[0]) + ' samples, each sized ' + str(train_data[0,:,:].shape) + '\\n')\n",
        "\n",
        "##############################################################################\n",
        "# Load data\n",
        "# data, label = load_data(data_dir, ALL_MOTION)\n",
        "# print('\\nLoaded dataset of ' + str(label.shape[0]) + ' samples, each sized ' + str(data[0,:,:].shape) + '\\n')\n",
        "\n",
        "# Split train and test\n",
        "# [data_train, data_test, label_train, label_test] = train_test_split(data, label, test_size=fraction_for_test)\n",
        "# print('\\nTrain on ' + str(label_train.shape[0]) + ' samples\\n' +\\\n",
        "    # 'Test on ' + str(label_test.shape[0]) + ' samples\\n')\n",
        "\n",
        "# One-hot encoding for train data\n",
        "# label_train = onehot_encoding(label_train, N_MOTION)\n",
        "\n",
        "# Load or fabricate model\n",
        "if use_existing_model:\n",
        "    model = load_model('model_Transformer_MultiAttention_LSTM_mse_1层Transformer_epoch200_86.61%.h5')\n",
        "    # model = load_model('model_LSTM_91.07_best_trained.h5')\n",
        "    # model = load_model('model_MultiAttention_LSTM_1层Transformer_MSE_epoch200_BS32_lr0.001_86.61%.h5')\n",
        "    # model = load_model('MultiAttention_LSTM_mse_1层Transformer_epoch50_79.46%.h5')\n",
        "    model.summary()\n",
        "else:\n",
        "    #model = assemble_model(input_shape=(T_MAX, 20, 20, 1), n_class=N_MOTION)\n",
        "    model = assemble_model(input_shape=(T_MAX, 15, 15, 1), n_class=N_MOTION) #修改点4\n",
        "    model.summary()\n",
        "\n",
        "class AccuracyLossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.acc = []\n",
        "        self.loss = []\n",
        "        self.val_acc = []\n",
        "        self.val_loss = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.acc.append(logs.get('accuracy'))\n",
        "        self.loss.append(logs.get('loss'))\n",
        "        self.val_acc.append(logs.get('val_accuracy'))\n",
        "        self.val_loss.append(logs.get('val_loss'))\n",
        "\n",
        "history = AccuracyLossHistory()\n",
        "\n",
        "# model.fit({'name_model_input': data_train},{'name_model_output': label_train},\n",
        "model.fit({'name_model_input': train_data},{'name_model_output': train_label},\n",
        "          batch_size=n_batch_size,\n",
        "          epochs=n_epochs,\n",
        "          verbose=1,\n",
        "          # validation_split=0.1,\n",
        "          shuffle=True, callbacks=[history])\n",
        "print('Saving trained model...')\n",
        "#model.save('model_widar3_trained.h5')\n",
        "model.save(f'{result_folder}/model.h5')  # 改动\n",
        "\n",
        "# Testing...\n",
        "print('Testing...')\n",
        "# 加载测试集数据\n",
        "test_data, test_label = load_data(data_dir_testing, ALL_MOTION)\n",
        "\n",
        "# 在评估模型性能时，使用测试集的数据进行评估\n",
        "loss, accuracy = model.evaluate(test_data, test_label)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# label_test_pred = model.predict(data_test)\n",
        "# label_test_pred = np.argmax(label_test_pred, axis = -1) + 2 #修改点5\n",
        "\n",
        "# Confusion Matrix\n",
        "# cm = confusion_matrix(label_test, label_test_pred)\n",
        "# print(cm)\n",
        "# cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
        "# cm = np.around(cm, decimals=2) # 保留几位小数的意思\n",
        "# print(cm)\n",
        "#cm_display = ConfusionMatrixDisplay(cm, display_labels=range(1, N_MOTION+1)).plot()\n",
        "#cm_display = ConfusionMatrixDisplay(cm, display_labels=range(4, N_MOTION+3)).plot() #2345 lebel-2  4567 4=2+2  7=长度4+3\n",
        "# cm_display = ConfusionMatrixDisplay(cm, display_labels=range(2, N_MOTION+2)).plot() #2345 lebel-2  4567 4=2+2  7=长度4+3\n",
        "# plt.show()\n",
        "# plt.savefig(f'{result_folder}/cm.png') # 新增点1 保存混淆矩阵结果2023.09.11\n",
        "\n",
        "# ########################################################\n",
        "# 添加其他指标\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "\n",
        "# 假设label_test_pred是模型的预测结果，label_test是真实的标签\n",
        "# classification_result = classification_report(label_test, label_test_pred, digits=4)\n",
        "\n",
        "# print(classification_result)\n",
        "\n",
        "# 将分类报告保存到文本文件\n",
        "# classification_file = os.path.join(result_folder, 'classification_report.txt')\n",
        "# with open(classification_file, 'w') as f:\n",
        "    # f.write(classification_result)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "# Accuracy 新增百分号\n",
        "# test_accuracy = np.sum(label_test == label_test_pred) / (label_test.shape[0])\n",
        "#print(test_accuracy)\n",
        "# test_accuracy_percentage = test_accuracy * 100\n",
        "# test_accuracy_str = f\"{test_accuracy_percentage:.2f}%\"\n",
        "# print(test_accuracy_str)\n",
        "\n",
        "# df_accuracy = pd.DataFrame({'Test Accuracy': [test_accuracy]})\n",
        "# df_accuracy.to_excel(f'{result_folder}/{test_accuracy_str}.xlsx', index=False) ## 新增点2 保存测试集平均准确率结果2023.09.11\n",
        "\n",
        "# df = pd.DataFrame({\n",
        "    # 'Epoch': range(1, n_epochs+1),\n",
        "    # 'Train Accuracy': history.acc,\n",
        "    # 'Train Loss': history.loss,\n",
        "    # 'Validation Accuracy': history.val_acc,\n",
        "    # 'Validation Loss': history.val_loss\n",
        "# })\n",
        "\n",
        "# df.to_excel('accuracy_l'''  '''oss.xlsx', index=False)\n",
        "# 保存准确率和损失数据到Excel表格中\n",
        "# df.to_excel(f'{result_folder}/accuracy_loss.xlsx', index=False) #改动\n",
        "\n",
        "# plt.plot(range(1, n_epochs+1), history.acc, label='Train Accuracy')\n",
        "# plt.plot(range(1, n_epochs+1), history.loss, label='Train Loss')\n",
        "# plt.plot(range(1, n_epochs+1), history.val_acc, label='Validation Accuracy')\n",
        "# plt.plot(range(1, n_epochs+1), history.val_loss, label='Validation Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend()\n",
        "# plt.savefig(f'{result_folder}/plot.png')\n",
        "# plt.show()\n",
        "\n",
        "# 新增 修改文件名 并打印新的文件名\n",
        "# new_result_folder = result_folder + f'_{test_accuracy_str}'\n",
        "# os.rename(result_folder, new_result_folder)\n",
        "# result_folder = new_result_folder\n",
        "\n",
        "# print(\"新的结果文件夹名称为：\", result_folder)\n",
        "\n",
        "end_time = time.time()  # 记录程序结束时间\n",
        "duration = end_time - start_time  # 计算程序运行时间\n",
        "print(\"程序执行时间为：{:.2f}秒\".format(duration))\n",
        "\n",
        "with open(f'{result_folder}/runtime.txt', 'w') as f:\n",
        "    f.write(\"程序执行时间为：{:.2f}秒\".format(duration)) # 新增点3 保存程序执行时间结果2023.09.11\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ujmBdVPZzU",
        "outputId": "1d37346b-a437-4abc-9a30-d89610647862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available\n",
            "\n",
            "Loaded dataset of 1120 samples, each sized (20, 15, 15, 1)\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " name_model_input (InputLay  [(None, 20, 15, 15, 1)]      0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " time_distributed (TimeDist  (None, 20, 11, 11, 16)       416       ['name_model_input[0][0]']    \n",
            " ributed)                                                                                         \n",
            "                                                                                                  \n",
            " time_distributed_1 (TimeDi  (None, 20, 5, 5, 16)         0         ['time_distributed[0][0]']    \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " time_distributed_2 (TimeDi  (None, 20, 400)              0         ['time_distributed_1[0][0]']  \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " time_distributed_3 (TimeDi  (None, 20, 64)               25664     ['time_distributed_2[0][0]']  \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " time_distributed_4 (TimeDi  (None, 20, 64)               0         ['time_distributed_3[0][0]']  \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " time_distributed_5 (TimeDi  (None, 20, 64)               4160      ['time_distributed_4[0][0]']  \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 20, 64)               16640     ['time_distributed_5[0][0]',  \n",
            " iHeadAttention)                                                     'time_distributed_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 20, 64)               0         ['multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 20, 64)               0         ['dropout_1[0][0]',           \n",
            " Lambda)                                                             'time_distributed_5[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 20, 64)               128       ['tf.__operators__.add[0][0]']\n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 20, 64)               4160      ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 20, 64)               4160      ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 20, 64)               0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 20, 64)               128       ['dropout_2[0][0]']           \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 128)                  98816     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 128)                  0         ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " name_model_output (Dense)   (None, 4)                    516       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 154788 (604.64 KB)\n",
            "Trainable params: 154788 (604.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "35/35 [==============================] - 4s 46ms/step - loss: 11.8994 - accuracy: 0.3223\n",
            "Epoch 2/30\n",
            "35/35 [==============================] - 2s 43ms/step - loss: 11.8248 - accuracy: 0.2286\n",
            "Epoch 3/30\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 11.8165 - accuracy: 0.2313\n",
            "Epoch 4/30\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 11.8138 - accuracy: 0.2571\n",
            "Epoch 5/30\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 11.8129 - accuracy: 0.2286\n",
            "Epoch 6/30\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 11.8126 - accuracy: 0.2580\n",
            "Epoch 7/30\n",
            "35/35 [==============================] - 2s 52ms/step - loss: 11.8125 - accuracy: 0.2741\n",
            "Epoch 8/30\n",
            "35/35 [==============================] - 1s 40ms/step - loss: 11.8125 - accuracy: 0.2786\n",
            "Epoch 9/30\n",
            "35/35 [==============================] - 1s 40ms/step - loss: 11.8125 - accuracy: 0.3223\n",
            "Epoch 10/30\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 11.8125 - accuracy: 0.3054\n",
            "Epoch 11/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2929\n",
            "Epoch 12/30\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 11.8125 - accuracy: 0.2643\n",
            "Epoch 13/30\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 11.8125 - accuracy: 0.2884\n",
            "Epoch 14/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2607\n",
            "Epoch 15/30\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 11.8125 - accuracy: 0.2661\n",
            "Epoch 16/30\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 11.8125 - accuracy: 0.2661\n",
            "Epoch 17/30\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 11.8125 - accuracy: 0.2536\n",
            "Epoch 18/30\n",
            "35/35 [==============================] - 1s 39ms/step - loss: 11.8125 - accuracy: 0.2616\n",
            "Epoch 19/30\n",
            "35/35 [==============================] - 1s 40ms/step - loss: 11.8125 - accuracy: 0.2518\n",
            "Epoch 20/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2562\n",
            "Epoch 21/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2214\n",
            "Epoch 22/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2804\n",
            "Epoch 23/30\n",
            "35/35 [==============================] - 2s 43ms/step - loss: 11.8125 - accuracy: 0.2116\n",
            "Epoch 24/30\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 11.8125 - accuracy: 0.2304\n",
            "Epoch 25/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2554\n",
            "Epoch 26/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2688\n",
            "Epoch 27/30\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 11.8125 - accuracy: 0.2795\n",
            "Epoch 28/30\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 11.8125 - accuracy: 0.2232\n",
            "Epoch 29/30\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 11.8125 - accuracy: 0.2402\n",
            "Epoch 30/30\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 11.8125 - accuracy: 0.2429\n",
            "Saving trained model...\n",
            "Testing...\n",
            "27/27 [==============================] - 1s 18ms/step - loss: 13.2292 - accuracy: 1.0000\n",
            "Test Accuracy: 1.0\n",
            "程序执行时间为：63.93秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 指定文件夹路径\n",
        "folder_path = \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4\"\n",
        "\n",
        "# 获取文件夹下所有子文件夹的名称\n",
        "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
        "\n",
        "# 遍历子文件夹，并重命名\n",
        "for subfolder in subfolders:\n",
        "    # 提取子文件夹的名称\n",
        "    folder_name = os.path.basename(subfolder)\n",
        "    # 构建新的文件夹名称，加上\"user\"前缀\n",
        "    new_folder_name = \"user\" + folder_name\n",
        "    # 构建新的文件夹路径\n",
        "    new_folder_path = os.path.join(folder_path, new_folder_name)\n",
        "    # 重命名文件夹\n",
        "    os.rename(subfolder, new_folder_path)\n",
        "    print(f\"已将文件夹 {folder_name} 重命名为 {new_folder_name}\")\n",
        "\n",
        "print(\"所有子文件夹重命名完成！\")\n"
      ],
      "metadata": {
        "id": "C1sJnUtJ9kw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4.zip\" -d \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "ubPGdKV27BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/DriverBVP[-1,1]bvplength20-分辨率15-手势4.zip\" -d \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "Jak9iQPv8t6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 先userx-后面动作的5改为6，然后3改为5\n",
        "\n",
        "import os\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user2'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 解析文件名，提取a和b的值\n",
        "    parts = filename.split('-')\n",
        "    a = int(parts[1])\n",
        "    b = int(parts[2])\n",
        "\n",
        "    #str.split(“o”)[0]得到的是第一个o之前的内容\n",
        "    #str.split(“o”)[1]得到的是第一个o和第二个o之间的内容\n",
        "    #str.split(“o”)[3]得到的是第三个o后和第四个o前之间的内容\n",
        "    #str.split(\"[\")[0]得到的是第一个 [ 之前的内容\n",
        "\n",
        "    # 第一步：将a=5的文件的a值改为6   #####################两步分次执行！！！\n",
        "    if a == 5:\n",
        "        new_a = 6\n",
        "    else:\n",
        "        new_a = a\n",
        "\n",
        "    # 第二步：将a=3的文件的a值改为5   #####################两步分次执行！！！\n",
        "    # if a == 3:\n",
        "        # new_a = 5\n",
        "    # else:\n",
        "        # new_a = a\n",
        "\n",
        "    # 构造新文件名\n",
        "    new_filename = f\"user2-{new_a}-{b}-1.mat\"\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "ufeaBfsM_jEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK\n",
        "import os\n",
        "################################################################################################################################\n",
        "# 执行复制操作\n",
        "!cp -r /content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4-BK/user1 /content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1\n",
        "\n",
        "# 检查目标文件夹是否存在\n",
        "target_folder = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1' # 此时的张梦数据更改了文件夹和文件前缀，加了user\n",
        "if os.path.exists(target_folder):\n",
        "    print(\"复制成功！\")\n",
        "else:\n",
        "    print(\"复制失败，请检查路径是否正确。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEg9OoKi_Bvz",
        "outputId": "cc940f22-7193-4d8a-b8d8-b5423faab657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "复制成功！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user14'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 构造新文件名，加上\"user\"前缀\n",
        "    new_filename = 'user' + filename\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "6EsIyEUD-TLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 去掉文件名中的所有下划线\"_\"\n",
        "    new_filename = filename.replace('_', '')\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "AK5ePxoi-kpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "# 删除多个文件夹\n",
        "import shutil\n",
        "# 要删除的文件夹路径列表\n",
        "folder_paths = [\n",
        "    #'/content/drive/My Drive/example_folder1',\n",
        "    #'/content/drive/My Drive/example_folder2',\n",
        "    #'/content/drive/My Drive/example_folder3',\n",
        "    # 添加更多的文件夹路径...\n",
        "    # test\n",
        "    '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user2',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user3',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user4',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user5',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user6',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user7',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user8',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user9',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user10',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user11',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user12',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user13',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user14',\n",
        "]\n",
        "# 遍历文件夹路径列表，逐个删除文件夹\n",
        "for folder_path in folder_paths:\n",
        "    try:\n",
        "        shutil.rmtree(folder_path)\n",
        "        print(f\"文件夹 '{folder_path}' 删除成功！\")\n",
        "    except Exception as e:\n",
        "        print(f\"删除文件夹 '{folder_path}' 时出现错误：{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpUpMZhk776E",
        "outputId": "04912fe0-7b06-41fb-93df-6e506083f101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件夹 '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1' 删除成功！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_INGrZ9z6iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "################################################################################################################################\n",
        "# 执行复制操作\n",
        "!cp -r /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK\n",
        "\n",
        "# 检查目标文件夹是否存在\n",
        "target_folder = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK'\n",
        "if os.path.exists(target_folder):\n",
        "    print(\"复制成功！\")\n",
        "else:\n",
        "    print(\"复制失败，请检查路径是否正确。\")\n"
      ],
      "metadata": {
        "id": "UYJ_yWJUyt5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "import shutil\n",
        "\n",
        "# 要删除的文件夹路径\n",
        "folder_to_delete = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK'\n",
        "\n",
        "# 使用 shutil.rmtree 递归删除文件夹及其内容\n",
        "shutil.rmtree(folder_to_delete)\n"
      ],
      "metadata": {
        "id": "vUUV9XGzx9HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}