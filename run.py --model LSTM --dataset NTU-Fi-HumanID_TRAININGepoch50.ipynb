{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1Cz2jHsRGlIY8OLCF5RPplMv55tD0VoXd",
      "authorship_tag": "ABX9TyMK3g1AYqXhwI+Gqpx/eYbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuo-liangliang/DeepLearning-check2023-2024/blob/master/run.py%20--model%20LSTM%20--dataset%20NTU-Fi-HumanID_TRAININGepoch50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oS3ZliHYxQ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/NTU-Fi_HAR.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi_HAR/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "WvsavS42Y8QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/NTU-Fi-HumanID.zip\" -d /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi-HumanID/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "w9lvbafDYurn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/UT_HAR.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/UT_HAR/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "XOjCBS_TDq4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "7hxwHXeDxR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koT1oiq_vVPj"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 删除指定的含有userx的数据\n",
        "################################################################################################################################\n",
        "import os\n",
        "\n",
        "# 定义目标文件夹路径\n",
        "# 1-3\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/1-Push&Pull'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/2-Sweep'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/3-Clap'\n",
        "\n",
        "# 4\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/4-Slide'\n",
        "\n",
        "# 5\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/5-Draw-N(H)'\n",
        "\n",
        "\n",
        "# 6-9\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/6-Draw-O(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/7-Draw-Rectangle(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/8-Draw-Triangle(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/9-Draw-Zigzag(H)'\n",
        "\n",
        "\n",
        "# 10-12\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/10-Draw-Zigzag(V)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/11-Draw-N(V)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/12-Draw-O(V)'\n",
        "\n",
        "\n",
        "# 13-22\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/13-Draw-1'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/14-Draw-2'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/15-Draw-3'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/16-Draw-4'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/17-Draw-5'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/18-Draw-6'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/19-Draw-7'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/20-Draw-8'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/21-Draw-9'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/22-Draw-10'\n",
        "\n",
        "# 获取目标文件夹中所有文件名\n",
        "files_before = os.listdir(folder_path)\n",
        "\n",
        "for file_name in files_before:\n",
        "    # 1-3\n",
        "    # 删除以'user3'到'user5'和'user8'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user4') or file_name.startswith('user5') or file_name.startswith('user8') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 4\n",
        "    # 删除以'user3'和'user5'和'user8'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user5') or file_name.startswith('user8') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 5\n",
        "    # 删除以'user3'和'user5'和'user9'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user5') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 6-9\n",
        "    # 删除以'user3'和'user9'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "\n",
        "    # 10-12\n",
        "    # 删除以'user2'到'user4'开头的文件\n",
        "    # if file_name.startswith('user2') or file_name.startswith('user3') or file_name.startswith('user4') :\n",
        "\n",
        "\n",
        "    # 13-22\n",
        "    # 删除以'user2'开头的文件\n",
        "    # if file_name.startswith('user2') :\n",
        "        os.remove(os.path.join(folder_path, file_name))\n",
        "\n",
        "\n",
        "# 再次获取目标文件夹中所有文件名\n",
        "files_after = os.listdir(folder_path)\n",
        "\n",
        "# 输出删除前后的文件数量\n",
        "print(\"删除前的文件数量:\", len(files_before))\n",
        "print(\"删除后的文件数量:\", len(files_after))\n",
        "\n"
      ],
      "metadata": {
        "id": "xiRzs_rbxvkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amX2d-r66deM",
        "outputId": "0dfc75eb-a62d-43fa-d2fd-00b2111f0a3f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "##############验证程序############验证程序###########验证程序##############验证程序########验证程序###########验证程序###########\n",
        "################################################################################################################################\n",
        "!pip install einops\n",
        "# !pip install torch==1.12.0 torchvision==0.13.0\n",
        "import torch\n",
        "import torchvision\n",
        "#!pip install torch torchvision torchaudio\n",
        "print(\"PyTorch 版本:\", torch.__version__)\n",
        "print(\"TorchVision 版本:\", torchvision.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV7neAy05cTe",
        "outputId": "2190fc20-4018-413d-9f3a-67c7ae9c0125"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "PyTorch 版本: 2.2.1+cu121\n",
            "TorchVision 版本: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main\"\n",
        "os.chdir(path)\n",
        "print(os.getcwd())\n",
        "# 更改运行目录，定位当前运行地址"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMHKhhQO5jgI",
        "outputId": "8c4d5245-8931-4b80-94bc-30218966ff25"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python run.py --model CNN+GRU --dataset Widar\n",
        "\n",
        "# !python run.py --model CNN+GRU --dataset UT_HAR_data\n",
        "# !python run.py --model GRU --dataset UT_HAR_data\n",
        "# !python run.py --model LSTM --dataset UT_HAR_data\n",
        "# !python run.py --model BiLSTM --dataset UT_HAR_data\n",
        "# !python run.py --model RNN --dataset UT_HAR_data\n",
        "# !python run.py --model ViT --dataset UT_HAR_data\n",
        "\n",
        "# !python run.py --model CNN+GRU --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model BiLSTM --dataset NTU-Fi-HumanID\n",
        "!python run.py --model LSTM --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model GRU --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model RNN --dataset NTU-Fi-HumanID\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vQk3ucx5lUN",
        "outputId": "f08d4235-0f50-4fb6-d7fa-adfa7afd5386"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using dataset: NTU-Fi-HumanID\n",
            "using model: LSTM\n",
            "Epoch:1, Accuracy:0.2267,Loss:2.4985\n",
            "Epoch:2, Accuracy:0.5025,Loss:2.0918\n",
            "Epoch:3, Accuracy:0.6184,Loss:1.7247\n",
            "Epoch:4, Accuracy:0.6533,Loss:1.4437\n",
            "Epoch:5, Accuracy:0.7123,Loss:1.2531\n",
            "Epoch:6, Accuracy:0.7827,Loss:1.0906\n",
            "Epoch:7, Accuracy:0.7900,Loss:1.0206\n",
            "Epoch:8, Accuracy:0.7892,Loss:0.9735\n",
            "Epoch:9, Accuracy:0.8035,Loss:0.8381\n",
            "Epoch:10, Accuracy:0.8378,Loss:0.7641\n",
            "Epoch:11, Accuracy:0.8721,Loss:0.6842\n",
            "Epoch:12, Accuracy:0.8712,Loss:0.6213\n",
            "Epoch:13, Accuracy:0.8635,Loss:0.5589\n",
            "Epoch:14, Accuracy:0.8693,Loss:0.5356\n",
            "Epoch:15, Accuracy:0.9088,Loss:0.4848\n",
            "Epoch:16, Accuracy:0.9414,Loss:0.4314\n",
            "Epoch:17, Accuracy:0.9258,Loss:0.3953\n",
            "Epoch:18, Accuracy:0.9358,Loss:0.3938\n",
            "Epoch:19, Accuracy:0.9431,Loss:0.3928\n",
            "Epoch:20, Accuracy:0.9171,Loss:0.4192\n",
            "Epoch:21, Accuracy:0.9175,Loss:0.4051\n",
            "Epoch:22, Accuracy:0.8593,Loss:0.5113\n",
            "Epoch:23, Accuracy:0.9186,Loss:0.4142\n",
            "Epoch:24, Accuracy:0.9256,Loss:0.3799\n",
            "Epoch:25, Accuracy:0.9672,Loss:0.2879\n",
            "Epoch:26, Accuracy:0.9809,Loss:0.2535\n",
            "Epoch:27, Accuracy:0.9757,Loss:0.2840\n",
            "Epoch:28, Accuracy:0.9670,Loss:0.2959\n",
            "Epoch:29, Accuracy:0.9603,Loss:0.2814\n",
            "Epoch:30, Accuracy:0.9744,Loss:0.2373\n",
            "Epoch:31, Accuracy:0.9809,Loss:0.2168\n",
            "Epoch:32, Accuracy:0.9846,Loss:0.2183\n",
            "Epoch:33, Accuracy:0.9931,Loss:0.1891\n",
            "Epoch:34, Accuracy:0.9846,Loss:0.1783\n",
            "Epoch:35, Accuracy:0.9828,Loss:0.1716\n",
            "Epoch:36, Accuracy:0.9896,Loss:0.1420\n",
            "Epoch:37, Accuracy:0.9794,Loss:0.1666\n",
            "Epoch:38, Accuracy:0.9383,Loss:0.2405\n",
            "Epoch:39, Accuracy:0.9601,Loss:0.2559\n",
            "Epoch:40, Accuracy:0.9672,Loss:0.2289\n",
            "Epoch:41, Accuracy:0.9811,Loss:0.1889\n",
            "Epoch:42, Accuracy:0.9898,Loss:0.1573\n",
            "Epoch:43, Accuracy:0.9965,Loss:0.1235\n",
            "Epoch:44, Accuracy:0.9933,Loss:0.1057\n",
            "Epoch:45, Accuracy:0.9983,Loss:0.0931\n",
            "Epoch:46, Accuracy:0.9950,Loss:0.0892\n",
            "Epoch:47, Accuracy:0.9965,Loss:0.0789\n",
            "Epoch:48, Accuracy:0.9983,Loss:0.0744\n",
            "Epoch:49, Accuracy:0.9983,Loss:0.0644\n",
            "Epoch:50, Accuracy:0.9965,Loss:0.0685\n",
            "Validation accuracy: 0.9885, Loss: 0.1002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定位到指定路径下\n",
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/SeptemberResult/result_DNN_CCE_epoch250')\n",
        "os.chdir('/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng')\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls5ZDn3v2AAV",
        "outputId": "d2e7157b-6038-48f3-de40-2062797d4250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEDRT-LSTM\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# time    : 2023/9/15 00:32\n",
        "# Author  : Liang-liang Zuo\n",
        "# @FileName: FEDRT-LSTM.py  DeepLearningBK-ZuoLiangliang/result_MultiAttention_LSTM_1层Transformer_MSE_epoch200_batchsize32_lr0.001_test0.1_95.54%.ipynb\n",
        "# @Software: Colab_Notebooks\n",
        "# 链接地址：https://github.com/zuo-liangliang/DeepLearningBK-ZuoLiangliang/blob/main/result_MultiAttention_LSTM_1%E5%B1%82Transformer_MSE_epoch200_batchsize32_lr0.001_test0.1_95.54%25.ipynb\n",
        "\n",
        "# 下载文件夹  计划下载的文件夹压成压缩包\n",
        "# import shutil\n",
        "\n",
        "# source_folder = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_Transformer_MultiAttention_LSTM_改costfunction为mse_1层Transformer_epoch200'  # 要压缩的文件夹路径\n",
        "# destination_zip = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/zip/result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_Transformer_MultiAttention_LSTM_改costfunction为mse_1层Transformer_epoch200'  # 压缩包保存路径\n",
        "\n",
        "# shutil.make_archive(destination_zip, 'zip', source_folder)\n",
        "\n",
        "# 定位到指定路径下\n",
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/SeptemberResult/result_MSE_epoch200_lr0.001_batchsize32所有网络汇总')\n",
        "# print(os.getcwd())\n",
        "\n",
        "# /content/drive/MyDrive/SeptemberResult/result_MSE_epoch200_lr0.001_batchsize32所有网络汇总\n",
        "\n",
        "# result_MultiAttention_LSTM_1层Transformer\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as scio\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.keras.backend.set_session\n",
        "import keras\n",
        "from keras.layers import Input, GRU, LSTM, Dense, Flatten, Dropout, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, TimeDistributed\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization  #加Transformer修改点1\n",
        "num_transformer_layers = 1  # 设置Transformer层数 #加Transformer修改点2  2层-->1层\n",
        "\n",
        "# Parameters\n",
        "# use_existing_model = False  # 创建一个模型（False）默认创建一个新模型\n",
        "use_existing_model = True    # 模型权重修改点 使用已经存在的模型（True）\n",
        "# data_dir = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/'   #修改数据集文件夹\n",
        "\n",
        "data_dir_training = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/'\n",
        "data_dir_testing = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/'  # 修改为另外环境下的数据路径\n",
        "\n",
        "#fraction_for_test = 0.1 # 修改测试集占比 原先是0.1-->0.2\n",
        "#ALL_MOTION = [1,2,3,4,5,6]\n",
        "ALL_MOTION = [2,3,4,5]\n",
        "N_MOTION = len(ALL_MOTION)\n",
        "T_MAX = 0\n",
        "n_epochs = 100       #原始是30 #修改点1 取在528上最好的一组超参\n",
        "f_dropout_ratio = 0.5\n",
        "n_gru_hidden_units = 32\n",
        "n_batch_size = 32     #原始是32-->16\n",
        "f_learning_rate = 0.0001  #原始是0.001-->0.002\n",
        "\n",
        "\n",
        "def normalize_data(data_1):\n",
        "    # data(ndarray)=>data_norm(ndarray): [20,20,T]=>[20,20,T]\n",
        "    data_1_max = np.concatenate((data_1.max(axis=0),data_1.max(axis=1)),axis=0).max(axis=0)\n",
        "    data_1_min = np.concatenate((data_1.min(axis=0),data_1.min(axis=1)),axis=0).min(axis=0)\n",
        "    if (len(np.where((data_1_max - data_1_min) == 0)[0]) > 0):\n",
        "        return data_1\n",
        "    data_1_max_rep = np.tile(data_1_max,(data_1.shape[0],data_1.shape[1],1))\n",
        "    data_1_min_rep = np.tile(data_1_min,(data_1.shape[0],data_1.shape[1],1))\n",
        "    data_1_norm = (data_1 - data_1_min_rep) / (data_1_max_rep - data_1_min_rep)\n",
        "    return  data_1_norm\n",
        "\n",
        "def zero_padding(data, T_MAX):\n",
        "    # data(list)=>data_pad(ndarray): [20,20,T1/T2/...]=>[20,20,T_MAX]\n",
        "    data_pad = []\n",
        "    for i in range(len(data)):\n",
        "        t = np.array(data[i]).shape[2]\n",
        "        data_pad.append(np.pad(data[i], ((0,0),(0,0),(T_MAX - t,0)), 'constant', constant_values = 0).tolist())\n",
        "    return np.array(data_pad)\n",
        "\n",
        "def onehot_encoding(label, num_class):\n",
        "    # label(list)=>_label(ndarray): [N,]=>[N,num_class]\n",
        "    label = np.array(label).astype('int32')\n",
        "    # assert (np.arange(0,np.unique(label).size)==np.unique(label)).prod()    # Check label from 0 to N\n",
        "    label = np.squeeze(label)\n",
        "    # _label = np.eye(num_class)[label-1]     # from label to onehot\n",
        "    _label = np.eye(num_class)[label-2]   #修改点2\n",
        "    return _label\n",
        "\n",
        "def load_data(path_to_data, motion_sel):\n",
        "    global T_MAX\n",
        "    data = []\n",
        "    label = []\n",
        "    for data_root, data_dirs, data_files in os.walk(path_to_data):\n",
        "        for data_file_name in data_files:\n",
        "\n",
        "            file_path = os.path.join(data_root,data_file_name)\n",
        "            try:\n",
        "                data_1 = scio.loadmat(file_path)['velocity_spectrum_ro']\n",
        "                label_1 = int(data_file_name.split('-')[1])\n",
        "                #location = int(data_file_name.split('-')[2])\n",
        "                #orientation = int(data_file_name.split('-')[3])\n",
        "                #repetition = int(data_file_name.split('-')[4])\n",
        "                repetition = int(data_file_name.split('-')[2])  #修改点3\n",
        "\n",
        "                # Select Motion\n",
        "                if (label_1 not in motion_sel):\n",
        "                    continue\n",
        "\n",
        "                # Select Location\n",
        "                # if (location not in [1,2,3,5]):\n",
        "                #     continue\n",
        "\n",
        "                # Select Orientation\n",
        "                # if (orientation not in [1,2,4,5]):\n",
        "                #     continue\n",
        "\n",
        "                # Normalization\n",
        "                data_normed_1 = normalize_data(data_1)\n",
        "\n",
        "                # Update T_MAX\n",
        "                if T_MAX < np.array(data_1).shape[2]:\n",
        "                    T_MAX = np.array(data_1).shape[2]\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # Save List\n",
        "            data.append(data_normed_1.tolist())\n",
        "            label.append(label_1)\n",
        "                # Zero-padding\n",
        "    data = zero_padding(data, T_MAX)\n",
        "\n",
        "    # Swap axes\n",
        "    data = np.swapaxes(np.swapaxes(data, 1, 3), 2, 3)   # [N,20,20',T_MAX]=>[N,T_MAX,20,20']\n",
        "    data = np.expand_dims(data, axis=-1)    # [N,T_MAX,20,20]=>[N,T_MAX,20,20,1]\n",
        "\n",
        "    # Convert label to ndarray\n",
        "    label = np.array(label)\n",
        "\n",
        "    # data(ndarray): [N,T_MAX,20,20,1], label(ndarray): [N,N_MOTION]\n",
        "    return data, label\n",
        "\n",
        "# 构建符合输入形状为 (None, 20, 15, 15, 1) 和输出形状为 (None, 4) 的  模型：\n",
        "\n",
        "#加Transformer修改点3\n",
        "def transformer_layer(inputs, hidden_units, dropout_rate):\n",
        "    # Self-attention\n",
        "    attention_output = MultiHeadAttention(\n",
        "        num_heads=8, key_dim=hidden_units // 8\n",
        "    )(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = LayerNormalization()(attention_output + inputs)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(hidden_units, activation=\"relu\")(attention_output)\n",
        "    ffn = Dense(hidden_units)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "    ffn = LayerNormalization()(ffn)\n",
        "\n",
        "    return ffn\n",
        "\n",
        "#加Transformer修改点4\n",
        "def assemble_model(input_shape, n_class):\n",
        "    model_input = Input(shape=input_shape, dtype='float32', name='name_model_input')    # (@,T_MAX,20,20,1)\n",
        "\n",
        "    # Feature extraction part\n",
        "    x = TimeDistributed(Conv2D(16,kernel_size=(5,5),activation='relu',data_format='channels_last',\\\n",
        "        input_shape=input_shape))(model_input)   # (@,T_MAX,20,20,1)=>(@,T_MAX,16,16,16)\n",
        "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)    # (@,T_MAX,16,16,16)=>(@,T_MAX,8,8,16)\n",
        "    x = TimeDistributed(Flatten())(x)   # (@,T_MAX,8,8,16)=>(@,T_MAX,8*8*16)\n",
        "    x = TimeDistributed(Dense(64,activation='relu'))(x) # (@,T_MAX,8*8*16)=>(@,T_MAX,64)\n",
        "    x = TimeDistributed(Dropout(f_dropout_ratio))(x)\n",
        "    x = TimeDistributed(Dense(64,activation='relu'))(x) # (@,T_MAX,64)=>(@,T_MAX,64)\n",
        "\n",
        "    # Add transformer layers\n",
        "    for _ in range(num_transformer_layers):\n",
        "        #x = transformer_layer(x, hidden_units=n_gru_hidden_units, dropout_rate=f_dropout_ratio)\n",
        "        x = transformer_layer(x, hidden_units=64, dropout_rate=f_dropout_ratio)  # 修改点1\n",
        "    x = LSTM(n_gru_hidden_units,return_sequences=False)(x)  # (@,T_MAX,64)=>(@,128)\n",
        "    x = Dropout(f_dropout_ratio)(x)\n",
        "    model_output = Dense(n_class, activation='softmax', name='name_model_output')(x)  # (@,128)=>(@,n_class)\n",
        "\n",
        "    # Create the model\n",
        "    # Model compiling\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(lr=f_learning_rate),\n",
        "                    # loss='categorical_crossentropy', # 将损失函数改为多类别交叉熵损失函数\n",
        "                    loss='mean_squared_error',  # 将损失函数改为均方误差\n",
        "                    # loss='mean_absolute_error',  # 将损失函数改为平均绝对误差\n",
        "                    # loss='binary_crossentropy',  # 将损失函数改为对数损失函数\n",
        "\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "    return model\n",
        "\n",
        "# 改\n",
        "#model_type = '双层LSTM'\n",
        "#model_type = 'LSTM'\n",
        "#model_type = 'BiLSTM'\n",
        "#model_type = 'MultiAttention_LSTM_1层Transformer'\n",
        "model_type = 'FEDRT-LSTM'\n",
        "\n",
        "# 创建结果文件夹\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_GRU_改costfunction为mse_epoch200' # 每次都要改\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_categorical_crossentropy_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_mean_squared_error_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_mean_absolute_error_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_binary_crossentropy_epoch{n_epochs}'\n",
        "\n",
        "# result_folder = f'result_{model_type}_CCE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "# result_folder = f'result_{model_type}_MSE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "result_folder = f'Result_528TRAIN-zhangmengTEST'\n",
        "\n",
        "# result_folder = f'result_{model_type}_MAE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}'\n",
        "# result_folder = f'result_{model_type}_BCE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "\n",
        "os.makedirs(result_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "RcrUiRWM2gKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's BEGIN >>>>\n",
        "\n",
        "import time\n",
        "start_time = time.time()  # 记录程序开始时间\n",
        "\n",
        "# 获取可见的GPU设备列表\n",
        "# visible_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# 检查是否存在可见的GPU设备\n",
        "# if len(visible_gpus) < 1:\n",
        "    # print('No GPU available')\n",
        "    # exit(0)\n",
        "# 设置使用第一个可见的GPU设备\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(visible_gpus[0].index)\n",
        "# 配置GPU选项\n",
        "# gpu_config = tf.config.experimental.set_memory_growth(visible_gpus[0], True)\n",
        "# 创建一个会话并应用GPU配置\n",
        "# sess = tf.compat.v1.Session(config=gpu_config)\n",
        "# tf.compat.v1.keras.backend.set_session(sess)\n",
        "# tf.random.set_seed(1)\n",
        "\n",
        "# 获取可见的 GPU 设备列表\n",
        "visible_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(visible_gpus) > 0:\n",
        "    # 如果有可见的 GPU 设备，设置 TensorFlow 在 GPU 上运行\n",
        "    tf.config.set_visible_devices(visible_gpus, 'GPU')\n",
        "    gpu_config = tf.config.experimental.set_memory_growth(visible_gpus[0], True)\n",
        "    sess = tf.compat.v1.Session(config=gpu_config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "    print('GPU available')\n",
        "else:\n",
        "    # 如果没有可见的 GPU 设备，设置 TensorFlow 在 CPU 上运行\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "    sess = tf.compat.v1.Session()\n",
        "    print('No GPU available')\n",
        "\n",
        "# 加载训练集数据\n",
        "train_data, train_label = load_data(data_dir_training, ALL_MOTION)\n",
        "print('\\nLoaded dataset of ' + str(train_label.shape[0]) + ' samples, each sized ' + str(train_data[0,:,:].shape) + '\\n')\n",
        "\n",
        "##############################################################################\n",
        "# Load data\n",
        "# data, label = load_data(data_dir, ALL_MOTION)\n",
        "# print('\\nLoaded dataset of ' + str(label.shape[0]) + ' samples, each sized ' + str(data[0,:,:].shape) + '\\n')\n",
        "\n",
        "# Split train and test\n",
        "# [data_train, data_test, label_train, label_test] = train_test_split(data, label, test_size=fraction_for_test)\n",
        "# print('\\nTrain on ' + str(label_train.shape[0]) + ' samples\\n' +\\\n",
        "    # 'Test on ' + str(label_test.shape[0]) + ' samples\\n')\n",
        "\n",
        "# One-hot encoding for train data\n",
        "# label_train = onehot_encoding(label_train, N_MOTION)\n",
        "\n",
        "# Load or fabricate model\n",
        "if use_existing_model:\n",
        "    model = load_model('model_Transformer_MultiAttention_LSTM_mse_1层Transformer_epoch200_86.61%.h5')\n",
        "    # model = load_model('model_LSTM_91.07_best_trained.h5')\n",
        "    # model = load_model('model_MultiAttention_LSTM_1层Transformer_MSE_epoch200_BS32_lr0.001_86.61%.h5')\n",
        "    # model = load_model('MultiAttention_LSTM_mse_1层Transformer_epoch50_79.46%.h5')\n",
        "    model.summary()\n",
        "else:\n",
        "    #model = assemble_model(input_shape=(T_MAX, 20, 20, 1), n_class=N_MOTION)\n",
        "    model = assemble_model(input_shape=(T_MAX, 15, 15, 1), n_class=N_MOTION) #修改点4\n",
        "    model.summary()\n",
        "\n",
        "class AccuracyLossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.acc = []\n",
        "        self.loss = []\n",
        "        self.val_acc = []\n",
        "        self.val_loss = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.acc.append(logs.get('accuracy'))\n",
        "        self.loss.append(logs.get('loss'))\n",
        "        self.val_acc.append(logs.get('val_accuracy'))\n",
        "        self.val_loss.append(logs.get('val_loss'))\n",
        "\n",
        "history = AccuracyLossHistory()\n",
        "\n",
        "# model.fit({'name_model_input': data_train},{'name_model_output': label_train},\n",
        "model.fit({'name_model_input': train_data},{'name_model_output': train_label},\n",
        "          batch_size=n_batch_size,\n",
        "          epochs=n_epochs,\n",
        "          verbose=1,\n",
        "          # validation_split=0.1,\n",
        "          shuffle=True, callbacks=[history])\n",
        "print('Saving trained model...')\n",
        "#model.save('model_widar3_trained.h5')\n",
        "model.save(f'{result_folder}/model.h5')  # 改动\n",
        "\n",
        "# Testing...\n",
        "print('Testing...')\n",
        "# 加载测试集数据\n",
        "test_data, test_label = load_data(data_dir_testing, ALL_MOTION)\n",
        "\n",
        "# 在评估模型性能时，使用测试集的数据进行评估\n",
        "loss, accuracy = model.evaluate(test_data, test_label)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# label_test_pred = model.predict(data_test)\n",
        "# label_test_pred = np.argmax(label_test_pred, axis = -1) + 2 #修改点5\n",
        "\n",
        "# Confusion Matrix\n",
        "# cm = confusion_matrix(label_test, label_test_pred)\n",
        "# print(cm)\n",
        "# cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
        "# cm = np.around(cm, decimals=2) # 保留几位小数的意思\n",
        "# print(cm)\n",
        "#cm_display = ConfusionMatrixDisplay(cm, display_labels=range(1, N_MOTION+1)).plot()\n",
        "#cm_display = ConfusionMatrixDisplay(cm, display_labels=range(4, N_MOTION+3)).plot() #2345 lebel-2  4567 4=2+2  7=长度4+3\n",
        "# cm_display = ConfusionMatrixDisplay(cm, display_labels=range(2, N_MOTION+2)).plot() #2345 lebel-2  4567 4=2+2  7=长度4+3\n",
        "# plt.show()\n",
        "# plt.savefig(f'{result_folder}/cm.png') # 新增点1 保存混淆矩阵结果2023.09.11\n",
        "\n",
        "# ########################################################\n",
        "# 添加其他指标\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "\n",
        "# 假设label_test_pred是模型的预测结果，label_test是真实的标签\n",
        "# classification_result = classification_report(label_test, label_test_pred, digits=4)\n",
        "\n",
        "# print(classification_result)\n",
        "\n",
        "# 将分类报告保存到文本文件\n",
        "# classification_file = os.path.join(result_folder, 'classification_report.txt')\n",
        "# with open(classification_file, 'w') as f:\n",
        "    # f.write(classification_result)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "# Accuracy 新增百分号\n",
        "# test_accuracy = np.sum(label_test == label_test_pred) / (label_test.shape[0])\n",
        "#print(test_accuracy)\n",
        "# test_accuracy_percentage = test_accuracy * 100\n",
        "# test_accuracy_str = f\"{test_accuracy_percentage:.2f}%\"\n",
        "# print(test_accuracy_str)\n",
        "\n",
        "# df_accuracy = pd.DataFrame({'Test Accuracy': [test_accuracy]})\n",
        "# df_accuracy.to_excel(f'{result_folder}/{test_accuracy_str}.xlsx', index=False) ## 新增点2 保存测试集平均准确率结果2023.09.11\n",
        "\n",
        "# df = pd.DataFrame({\n",
        "    # 'Epoch': range(1, n_epochs+1),\n",
        "    # 'Train Accuracy': history.acc,\n",
        "    # 'Train Loss': history.loss,\n",
        "    # 'Validation Accuracy': history.val_acc,\n",
        "    # 'Validation Loss': history.val_loss\n",
        "# })\n",
        "\n",
        "# df.to_excel('accuracy_l'''  '''oss.xlsx', index=False)\n",
        "# 保存准确率和损失数据到Excel表格中\n",
        "# df.to_excel(f'{result_folder}/accuracy_loss.xlsx', index=False) #改动\n",
        "\n",
        "# plt.plot(range(1, n_epochs+1), history.acc, label='Train Accuracy')\n",
        "# plt.plot(range(1, n_epochs+1), history.loss, label='Train Loss')\n",
        "# plt.plot(range(1, n_epochs+1), history.val_acc, label='Validation Accuracy')\n",
        "# plt.plot(range(1, n_epochs+1), history.val_loss, label='Validation Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend()\n",
        "# plt.savefig(f'{result_folder}/plot.png')\n",
        "# plt.show()\n",
        "\n",
        "# 新增 修改文件名 并打印新的文件名\n",
        "# new_result_folder = result_folder + f'_{test_accuracy_str}'\n",
        "# os.rename(result_folder, new_result_folder)\n",
        "# result_folder = new_result_folder\n",
        "\n",
        "# print(\"新的结果文件夹名称为：\", result_folder)\n",
        "\n",
        "end_time = time.time()  # 记录程序结束时间\n",
        "duration = end_time - start_time  # 计算程序运行时间\n",
        "print(\"程序执行时间为：{:.2f}秒\".format(duration))\n",
        "\n",
        "with open(f'{result_folder}/runtime.txt', 'w') as f:\n",
        "    f.write(\"程序执行时间为：{:.2f}秒\".format(duration)) # 新增点3 保存程序执行时间结果2023.09.11\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_ujmBdVPZzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 指定文件夹路径\n",
        "folder_path = \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4\"\n",
        "\n",
        "# 获取文件夹下所有子文件夹的名称\n",
        "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
        "\n",
        "# 遍历子文件夹，并重命名\n",
        "for subfolder in subfolders:\n",
        "    # 提取子文件夹的名称\n",
        "    folder_name = os.path.basename(subfolder)\n",
        "    # 构建新的文件夹名称，加上\"user\"前缀\n",
        "    new_folder_name = \"user\" + folder_name\n",
        "    # 构建新的文件夹路径\n",
        "    new_folder_path = os.path.join(folder_path, new_folder_name)\n",
        "    # 重命名文件夹\n",
        "    os.rename(subfolder, new_folder_path)\n",
        "    print(f\"已将文件夹 {folder_name} 重命名为 {new_folder_name}\")\n",
        "\n",
        "print(\"所有子文件夹重命名完成！\")\n"
      ],
      "metadata": {
        "id": "C1sJnUtJ9kw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4.zip\" -d \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "ubPGdKV27BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/DriverBVP[-1,1]bvplength20-分辨率15-手势4.zip\" -d \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "Jak9iQPv8t6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 先userx-后面动作的5改为6，然后3改为5\n",
        "\n",
        "import os\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user2'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 解析文件名，提取a和b的值\n",
        "    parts = filename.split('-')\n",
        "    a = int(parts[1])\n",
        "    b = int(parts[2])\n",
        "\n",
        "    #str.split(“o”)[0]得到的是第一个o之前的内容\n",
        "    #str.split(“o”)[1]得到的是第一个o和第二个o之间的内容\n",
        "    #str.split(“o”)[3]得到的是第三个o后和第四个o前之间的内容\n",
        "    #str.split(\"[\")[0]得到的是第一个 [ 之前的内容\n",
        "\n",
        "    # 第一步：将a=5的文件的a值改为6   #####################两步分次执行！！！\n",
        "    if a == 5:\n",
        "        new_a = 6\n",
        "    else:\n",
        "        new_a = a\n",
        "\n",
        "    # 第二步：将a=3的文件的a值改为5   #####################两步分次执行！！！\n",
        "    # if a == 3:\n",
        "        # new_a = 5\n",
        "    # else:\n",
        "        # new_a = a\n",
        "\n",
        "    # 构造新文件名\n",
        "    new_filename = f\"user2-{new_a}-{b}-1.mat\"\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "ufeaBfsM_jEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK\n",
        "import os\n",
        "################################################################################################################################\n",
        "# 执行复制操作\n",
        "!cp -r /content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4-BK/user1 /content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1\n",
        "\n",
        "# 检查目标文件夹是否存在\n",
        "target_folder = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1' # 此时的张梦数据更改了文件夹和文件前缀，加了user\n",
        "if os.path.exists(target_folder):\n",
        "    print(\"复制成功！\")\n",
        "else:\n",
        "    print(\"复制失败，请检查路径是否正确。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEg9OoKi_Bvz",
        "outputId": "cc940f22-7193-4d8a-b8d8-b5423faab657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "复制成功！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user14'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 构造新文件名，加上\"user\"前缀\n",
        "    new_filename = 'user' + filename\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "6EsIyEUD-TLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 去掉文件名中的所有下划线\"_\"\n",
        "    new_filename = filename.replace('_', '')\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "AK5ePxoi-kpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "# 删除多个文件夹\n",
        "import shutil\n",
        "# 要删除的文件夹路径列表\n",
        "folder_paths = [\n",
        "    #'/content/drive/My Drive/example_folder1',\n",
        "    #'/content/drive/My Drive/example_folder2',\n",
        "    #'/content/drive/My Drive/example_folder3',\n",
        "    # 添加更多的文件夹路径...\n",
        "    # test\n",
        "    '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi_HAR',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user2',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user3',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user4',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user5',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user6',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user7',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user8',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user9',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user10',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user11',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user12',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user13',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user14',\n",
        "]\n",
        "# 遍历文件夹路径列表，逐个删除文件夹\n",
        "for folder_path in folder_paths:\n",
        "    try:\n",
        "        shutil.rmtree(folder_path)\n",
        "        print(f\"文件夹 '{folder_path}' 删除成功！\")\n",
        "    except Exception as e:\n",
        "        print(f\"删除文件夹 '{folder_path}' 时出现错误：{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpUpMZhk776E",
        "outputId": "537388e3-88a4-49b0-8c09-2988773d2394"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件夹 '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi_HAR' 删除成功！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_INGrZ9z6iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "################################################################################################################################\n",
        "# 执行复制操作\n",
        "!cp -r /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK\n",
        "\n",
        "# 检查目标文件夹是否存在\n",
        "target_folder = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK'\n",
        "if os.path.exists(target_folder):\n",
        "    print(\"复制成功！\")\n",
        "else:\n",
        "    print(\"复制失败，请检查路径是否正确。\")\n"
      ],
      "metadata": {
        "id": "UYJ_yWJUyt5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "import shutil\n",
        "\n",
        "# 要删除的文件夹路径\n",
        "folder_to_delete = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK'\n",
        "\n",
        "# 使用 shutil.rmtree 递归删除文件夹及其内容\n",
        "shutil.rmtree(folder_to_delete)\n"
      ],
      "metadata": {
        "id": "vUUV9XGzx9HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}