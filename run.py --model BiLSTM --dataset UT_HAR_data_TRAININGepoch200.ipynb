{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1Cz2jHsRGlIY8OLCF5RPplMv55tD0VoXd",
      "authorship_tag": "ABX9TyOF/ui0qKfJGPgkoelZeCD+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuo-liangliang/DeepLearning-check2023-2024/blob/check-yjf-2024.04.22/run.py%20--model%20BiLSTM%20--dataset%20UT_HAR_data_TRAININGepoch200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oS3ZliHYxQ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/NTU-Fi_HAR.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi_HAR/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "WvsavS42Y8QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/NTU-Fi-HumanID.zip\" -d /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi-HumanID/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "w9lvbafDYurn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/UT_HAR.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/UT_HAR/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "XOjCBS_TDq4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "7hxwHXeDxR14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koT1oiq_vVPj"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata.zip\" -d \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 删除指定的含有userx的数据\n",
        "################################################################################################################################\n",
        "import os\n",
        "\n",
        "# 定义目标文件夹路径\n",
        "# 1-3\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/1-Push&Pull'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/2-Sweep'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/3-Clap'\n",
        "\n",
        "# 4\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/4-Slide'\n",
        "\n",
        "# 5\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/5-Draw-N(H)'\n",
        "\n",
        "\n",
        "# 6-9\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/6-Draw-O(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/7-Draw-Rectangle(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/8-Draw-Triangle(H)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/9-Draw-Zigzag(H)'\n",
        "\n",
        "\n",
        "# 10-12\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/10-Draw-Zigzag(V)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/11-Draw-N(V)'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/12-Draw-O(V)'\n",
        "\n",
        "\n",
        "# 13-22\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/13-Draw-1'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/14-Draw-2'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/15-Draw-3'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/16-Draw-4'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/17-Draw-5'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/18-Draw-6'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/19-Draw-7'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/20-Draw-8'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/21-Draw-9'\n",
        "# folder_path = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata/test/22-Draw-10'\n",
        "\n",
        "# 获取目标文件夹中所有文件名\n",
        "files_before = os.listdir(folder_path)\n",
        "\n",
        "for file_name in files_before:\n",
        "    # 1-3\n",
        "    # 删除以'user3'到'user5'和'user8'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user4') or file_name.startswith('user5') or file_name.startswith('user8') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 4\n",
        "    # 删除以'user3'和'user5'和'user8'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user5') or file_name.startswith('user8') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 5\n",
        "    # 删除以'user3'和'user5'和'user9'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user5') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "    # 6-9\n",
        "    # 删除以'user3'和'user9'到'user17'开头的文件\n",
        "    # if file_name.startswith('user3') or file_name.startswith('user9') or file_name.startswith('user10') or file_name.startswith('user11') or file_name.startswith('user12') or file_name.startswith('user13') or file_name.startswith('user14') or file_name.startswith('user15') or file_name.startswith('user16') or file_name.startswith('user17'):\n",
        "\n",
        "\n",
        "    # 10-12\n",
        "    # 删除以'user2'到'user4'开头的文件\n",
        "    # if file_name.startswith('user2') or file_name.startswith('user3') or file_name.startswith('user4') :\n",
        "\n",
        "\n",
        "    # 13-22\n",
        "    # 删除以'user2'开头的文件\n",
        "    # if file_name.startswith('user2') :\n",
        "        os.remove(os.path.join(folder_path, file_name))\n",
        "\n",
        "\n",
        "# 再次获取目标文件夹中所有文件名\n",
        "files_after = os.listdir(folder_path)\n",
        "\n",
        "# 输出删除前后的文件数量\n",
        "print(\"删除前的文件数量:\", len(files_before))\n",
        "print(\"删除后的文件数量:\", len(files_after))\n",
        "\n"
      ],
      "metadata": {
        "id": "xiRzs_rbxvkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amX2d-r66deM",
        "outputId": "0dfc75eb-a62d-43fa-d2fd-00b2111f0a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "##############验证程序############验证程序###########验证程序##############验证程序########验证程序###########验证程序###########\n",
        "################################################################################################################################\n",
        "!pip install einops\n",
        "# !pip install torch==1.12.0 torchvision==0.13.0\n",
        "import torch\n",
        "import torchvision\n",
        "#!pip install torch torchvision torchaudio\n",
        "print(\"PyTorch 版本:\", torch.__version__)\n",
        "print(\"TorchVision 版本:\", torchvision.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV7neAy05cTe",
        "outputId": "7156feaf-f4e5-4a23-fd9d-f76199134276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n",
            "PyTorch 版本: 2.2.1+cu121\n",
            "TorchVision 版本: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main\"\n",
        "os.chdir(path)\n",
        "print(os.getcwd())\n",
        "# 更改运行目录，定位当前运行地址"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMHKhhQO5jgI",
        "outputId": "4acaefdf-5c3b-4e21-ded7-eadb2f53c97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch=100\n",
        "# !python run.py --model CNN+GRU --dataset Widar\n",
        "\n",
        "# 第一次epoch=100 第2次 epoch=200\n",
        "# !python run.py --model CNN+GRU --dataset UT_HAR_data\n",
        "# !python run.py --model GRU --dataset UT_HAR_data\n",
        "# !python run.py --model LSTM --dataset UT_HAR_data\n",
        "!python run.py --model BiLSTM --dataset UT_HAR_data\n",
        "# !python run.py --model RNN --dataset UT_HAR_data\n",
        "# !python run.py --model ViT --dataset UT_HAR_data\n",
        "\n",
        "# !python run.py --model CNN+GRU --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model BiLSTM --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model LSTM --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model GRU --dataset NTU-Fi-HumanID\n",
        "# !python run.py --model RNN --dataset NTU-Fi-HumanID\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vQk3ucx5lUN",
        "outputId": "de21dac4-1809-40d6-beec-12513bf24d60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using dataset: UT-HAR DATA\n",
            "using model: BiLSTM\n",
            "Epoch:1, Accuracy:0.3102,Loss:1.7577\n",
            "Epoch:2, Accuracy:0.3334,Loss:1.7054\n",
            "Epoch:3, Accuracy:0.3188,Loss:1.7546\n",
            "Epoch:4, Accuracy:0.3561,Loss:1.6773\n",
            "Epoch:5, Accuracy:0.3314,Loss:1.7169\n",
            "Epoch:6, Accuracy:0.3566,Loss:1.6939\n",
            "Epoch:7, Accuracy:0.3402,Loss:1.7036\n",
            "Epoch:8, Accuracy:0.3634,Loss:1.6631\n",
            "Epoch:9, Accuracy:0.3478,Loss:1.6637\n",
            "Epoch:10, Accuracy:0.3702,Loss:1.6486\n",
            "Epoch:11, Accuracy:0.3251,Loss:1.7176\n",
            "Epoch:12, Accuracy:0.3405,Loss:1.6949\n",
            "Epoch:13, Accuracy:0.3614,Loss:1.6836\n",
            "Epoch:14, Accuracy:0.3816,Loss:1.6091\n",
            "Epoch:15, Accuracy:0.2755,Loss:1.7980\n",
            "Epoch:16, Accuracy:0.3185,Loss:1.6763\n",
            "Epoch:17, Accuracy:0.3778,Loss:1.6164\n",
            "Epoch:18, Accuracy:0.3770,Loss:1.5888\n",
            "Epoch:19, Accuracy:0.4163,Loss:1.5569\n",
            "Epoch:20, Accuracy:0.4403,Loss:1.5289\n",
            "Epoch:21, Accuracy:0.4304,Loss:1.5337\n",
            "Epoch:22, Accuracy:0.4504,Loss:1.5107\n",
            "Epoch:23, Accuracy:0.4415,Loss:1.5017\n",
            "Epoch:24, Accuracy:0.4320,Loss:1.5299\n",
            "Epoch:25, Accuracy:0.4405,Loss:1.5047\n",
            "Epoch:26, Accuracy:0.4599,Loss:1.4824\n",
            "Epoch:27, Accuracy:0.4569,Loss:1.4772\n",
            "Epoch:28, Accuracy:0.4380,Loss:1.5020\n",
            "Epoch:29, Accuracy:0.4476,Loss:1.4782\n",
            "Epoch:30, Accuracy:0.4826,Loss:1.4326\n",
            "Epoch:31, Accuracy:0.4808,Loss:1.4191\n",
            "Epoch:32, Accuracy:0.4922,Loss:1.4009\n",
            "Epoch:33, Accuracy:0.4801,Loss:1.4139\n",
            "Epoch:34, Accuracy:0.4977,Loss:1.3950\n",
            "Epoch:35, Accuracy:0.5108,Loss:1.3693\n",
            "Epoch:36, Accuracy:0.5186,Loss:1.3559\n",
            "Epoch:37, Accuracy:0.5118,Loss:1.3472\n",
            "Epoch:38, Accuracy:0.5310,Loss:1.3135\n",
            "Epoch:39, Accuracy:0.5315,Loss:1.2838\n",
            "Epoch:40, Accuracy:0.5391,Loss:1.2643\n",
            "Epoch:41, Accuracy:0.5325,Loss:1.2560\n",
            "Epoch:42, Accuracy:0.5499,Loss:1.2433\n",
            "Epoch:43, Accuracy:0.5630,Loss:1.2123\n",
            "Epoch:44, Accuracy:0.5557,Loss:1.2093\n",
            "Epoch:45, Accuracy:0.5801,Loss:1.1652\n",
            "Epoch:46, Accuracy:0.5680,Loss:1.1931\n",
            "Epoch:47, Accuracy:0.5859,Loss:1.1621\n",
            "Epoch:48, Accuracy:0.5809,Loss:1.1318\n",
            "Epoch:49, Accuracy:0.5804,Loss:1.1408\n",
            "Epoch:50, Accuracy:0.5585,Loss:1.2001\n",
            "Epoch:51, Accuracy:0.5998,Loss:1.0864\n",
            "Epoch:52, Accuracy:0.5834,Loss:1.1232\n",
            "Epoch:53, Accuracy:0.6038,Loss:1.0649\n",
            "Epoch:54, Accuracy:0.5932,Loss:1.0667\n",
            "Epoch:55, Accuracy:0.5461,Loss:1.1977\n",
            "Epoch:56, Accuracy:0.5242,Loss:1.2819\n",
            "Epoch:57, Accuracy:0.6081,Loss:1.0927\n",
            "Epoch:58, Accuracy:0.6033,Loss:1.0813\n",
            "Epoch:59, Accuracy:0.6053,Loss:1.0628\n",
            "Epoch:60, Accuracy:0.6071,Loss:1.0713\n",
            "Epoch:61, Accuracy:0.6006,Loss:1.0651\n",
            "Epoch:62, Accuracy:0.6058,Loss:1.0748\n",
            "Epoch:63, Accuracy:0.6195,Loss:1.0244\n",
            "Epoch:64, Accuracy:0.6104,Loss:1.0518\n",
            "Epoch:65, Accuracy:0.6147,Loss:1.0271\n",
            "Epoch:66, Accuracy:0.6232,Loss:1.0134\n",
            "Epoch:67, Accuracy:0.6338,Loss:0.9815\n",
            "Epoch:68, Accuracy:0.6406,Loss:0.9556\n",
            "Epoch:69, Accuracy:0.6532,Loss:0.9359\n",
            "Epoch:70, Accuracy:0.6588,Loss:0.9399\n",
            "Epoch:71, Accuracy:0.6505,Loss:0.9218\n",
            "Epoch:72, Accuracy:0.6444,Loss:0.9432\n",
            "Epoch:73, Accuracy:0.6492,Loss:0.9358\n",
            "Epoch:74, Accuracy:0.6573,Loss:0.9122\n",
            "Epoch:75, Accuracy:0.6522,Loss:0.9391\n",
            "Epoch:76, Accuracy:0.6668,Loss:0.9032\n",
            "Epoch:77, Accuracy:0.6749,Loss:0.8717\n",
            "Epoch:78, Accuracy:0.6734,Loss:0.8610\n",
            "Epoch:79, Accuracy:0.6673,Loss:0.9059\n",
            "Epoch:80, Accuracy:0.6729,Loss:0.8677\n",
            "Epoch:81, Accuracy:0.6741,Loss:0.8780\n",
            "Epoch:82, Accuracy:0.6867,Loss:0.8424\n",
            "Epoch:83, Accuracy:0.6673,Loss:0.8875\n",
            "Epoch:84, Accuracy:0.6754,Loss:0.8528\n",
            "Epoch:85, Accuracy:0.6910,Loss:0.8274\n",
            "Epoch:86, Accuracy:0.6925,Loss:0.8091\n",
            "Epoch:87, Accuracy:0.6779,Loss:0.8503\n",
            "Epoch:88, Accuracy:0.6973,Loss:0.8064\n",
            "Epoch:89, Accuracy:0.6991,Loss:0.7918\n",
            "Epoch:90, Accuracy:0.6958,Loss:0.7948\n",
            "Epoch:91, Accuracy:0.7004,Loss:0.7821\n",
            "Epoch:92, Accuracy:0.6139,Loss:1.0482\n",
            "Epoch:93, Accuracy:0.6852,Loss:0.8493\n",
            "Epoch:94, Accuracy:0.7029,Loss:0.8005\n",
            "Epoch:95, Accuracy:0.7185,Loss:0.7752\n",
            "Epoch:96, Accuracy:0.6910,Loss:0.8315\n",
            "Epoch:97, Accuracy:0.7031,Loss:0.8017\n",
            "Epoch:98, Accuracy:0.7059,Loss:0.7651\n",
            "Epoch:99, Accuracy:0.7135,Loss:0.7551\n",
            "Epoch:100, Accuracy:0.7145,Loss:0.7592\n",
            "Epoch:101, Accuracy:0.7077,Loss:0.7607\n",
            "Epoch:102, Accuracy:0.7220,Loss:0.7410\n",
            "Epoch:103, Accuracy:0.7092,Loss:0.7701\n",
            "Epoch:104, Accuracy:0.7288,Loss:0.7301\n",
            "Epoch:105, Accuracy:0.7296,Loss:0.7165\n",
            "Epoch:106, Accuracy:0.7215,Loss:0.7395\n",
            "Epoch:107, Accuracy:0.7321,Loss:0.7310\n",
            "Epoch:108, Accuracy:0.7195,Loss:0.7477\n",
            "Epoch:109, Accuracy:0.7462,Loss:0.6922\n",
            "Epoch:110, Accuracy:0.7374,Loss:0.6928\n",
            "Epoch:111, Accuracy:0.7417,Loss:0.6952\n",
            "Epoch:112, Accuracy:0.7523,Loss:0.6881\n",
            "Epoch:113, Accuracy:0.7550,Loss:0.6638\n",
            "Epoch:114, Accuracy:0.7588,Loss:0.6730\n",
            "Epoch:115, Accuracy:0.7419,Loss:0.7095\n",
            "Epoch:116, Accuracy:0.7515,Loss:0.6833\n",
            "Epoch:117, Accuracy:0.7596,Loss:0.6568\n",
            "Epoch:118, Accuracy:0.7586,Loss:0.6811\n",
            "Epoch:119, Accuracy:0.7545,Loss:0.6644\n",
            "Epoch:120, Accuracy:0.7664,Loss:0.6293\n",
            "Epoch:121, Accuracy:0.7825,Loss:0.6058\n",
            "Epoch:122, Accuracy:0.7623,Loss:0.6712\n",
            "Epoch:123, Accuracy:0.7714,Loss:0.6280\n",
            "Epoch:124, Accuracy:0.7845,Loss:0.6193\n",
            "Epoch:125, Accuracy:0.7888,Loss:0.5825\n",
            "Epoch:126, Accuracy:0.7949,Loss:0.5855\n",
            "Epoch:127, Accuracy:0.7941,Loss:0.5926\n",
            "Epoch:128, Accuracy:0.8034,Loss:0.5838\n",
            "Epoch:129, Accuracy:0.7896,Loss:0.5924\n",
            "Epoch:130, Accuracy:0.8072,Loss:0.5511\n",
            "Epoch:131, Accuracy:0.8153,Loss:0.5368\n",
            "Epoch:132, Accuracy:0.8130,Loss:0.5419\n",
            "Epoch:133, Accuracy:0.8117,Loss:0.5370\n",
            "Epoch:134, Accuracy:0.5859,Loss:1.1728\n",
            "Epoch:135, Accuracy:0.6913,Loss:0.8223\n",
            "Epoch:136, Accuracy:0.7886,Loss:0.5915\n",
            "Epoch:137, Accuracy:0.8107,Loss:0.5636\n",
            "Epoch:138, Accuracy:0.8143,Loss:0.5443\n",
            "Epoch:139, Accuracy:0.8266,Loss:0.5165\n",
            "Epoch:140, Accuracy:0.8322,Loss:0.4936\n",
            "Epoch:141, Accuracy:0.8120,Loss:0.5226\n",
            "Epoch:142, Accuracy:0.8329,Loss:0.4944\n",
            "Epoch:143, Accuracy:0.8185,Loss:0.5330\n",
            "Epoch:144, Accuracy:0.8478,Loss:0.4650\n",
            "Epoch:145, Accuracy:0.8233,Loss:0.5185\n",
            "Epoch:146, Accuracy:0.8236,Loss:0.5223\n",
            "Epoch:147, Accuracy:0.8392,Loss:0.4695\n",
            "Epoch:148, Accuracy:0.8390,Loss:0.4709\n",
            "Epoch:149, Accuracy:0.8367,Loss:0.4789\n",
            "Epoch:150, Accuracy:0.8417,Loss:0.4692\n",
            "Epoch:151, Accuracy:0.8208,Loss:0.5237\n",
            "Epoch:152, Accuracy:0.8402,Loss:0.4657\n",
            "Epoch:153, Accuracy:0.8591,Loss:0.4288\n",
            "Epoch:154, Accuracy:0.8359,Loss:0.4650\n",
            "Epoch:155, Accuracy:0.8445,Loss:0.4547\n",
            "Epoch:156, Accuracy:0.8095,Loss:0.5588\n",
            "Epoch:157, Accuracy:0.8354,Loss:0.4900\n",
            "Epoch:158, Accuracy:0.8584,Loss:0.4232\n",
            "Epoch:159, Accuracy:0.8438,Loss:0.4620\n",
            "Epoch:160, Accuracy:0.8624,Loss:0.4092\n",
            "Epoch:161, Accuracy:0.8591,Loss:0.4225\n",
            "Epoch:162, Accuracy:0.8599,Loss:0.4273\n",
            "Epoch:163, Accuracy:0.8621,Loss:0.4082\n",
            "Epoch:164, Accuracy:0.8669,Loss:0.3928\n",
            "Epoch:165, Accuracy:0.8642,Loss:0.4001\n",
            "Epoch:166, Accuracy:0.8354,Loss:0.4774\n",
            "Epoch:167, Accuracy:0.8319,Loss:0.4970\n",
            "Epoch:168, Accuracy:0.8543,Loss:0.4303\n",
            "Epoch:169, Accuracy:0.8793,Loss:0.3568\n",
            "Epoch:170, Accuracy:0.8697,Loss:0.3862\n",
            "Epoch:171, Accuracy:0.8735,Loss:0.3737\n",
            "Epoch:172, Accuracy:0.8669,Loss:0.3938\n",
            "Epoch:173, Accuracy:0.8805,Loss:0.3625\n",
            "Epoch:174, Accuracy:0.8758,Loss:0.3627\n",
            "Epoch:175, Accuracy:0.8856,Loss:0.3488\n",
            "Epoch:176, Accuracy:0.8747,Loss:0.3759\n",
            "Epoch:177, Accuracy:0.8609,Loss:0.4053\n",
            "Epoch:178, Accuracy:0.8629,Loss:0.3927\n",
            "Epoch:179, Accuracy:0.8790,Loss:0.3612\n",
            "Epoch:180, Accuracy:0.8725,Loss:0.3757\n",
            "Epoch:181, Accuracy:0.8916,Loss:0.3257\n",
            "Epoch:182, Accuracy:0.8899,Loss:0.3348\n",
            "Epoch:183, Accuracy:0.8616,Loss:0.3943\n",
            "Epoch:184, Accuracy:0.8705,Loss:0.3642\n",
            "Epoch:185, Accuracy:0.8911,Loss:0.3245\n",
            "Epoch:186, Accuracy:0.8750,Loss:0.3554\n",
            "Epoch:187, Accuracy:0.8999,Loss:0.3105\n",
            "Epoch:188, Accuracy:0.8410,Loss:0.4549\n",
            "Epoch:189, Accuracy:0.8934,Loss:0.3369\n",
            "Epoch:190, Accuracy:0.8914,Loss:0.3217\n",
            "Epoch:191, Accuracy:0.8931,Loss:0.3193\n",
            "Epoch:192, Accuracy:0.9025,Loss:0.2966\n",
            "Epoch:193, Accuracy:0.9083,Loss:0.2847\n",
            "Epoch:194, Accuracy:0.9007,Loss:0.2898\n",
            "Epoch:195, Accuracy:0.9017,Loss:0.3038\n",
            "Epoch:196, Accuracy:0.8690,Loss:0.4004\n",
            "Epoch:197, Accuracy:0.8095,Loss:0.5723\n",
            "Epoch:198, Accuracy:0.8982,Loss:0.3265\n",
            "Epoch:199, Accuracy:0.8916,Loss:0.3230\n",
            "Epoch:200, Accuracy:0.9027,Loss:0.2992\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/run.py\", line 147, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/run.py\", line 137, in main\n",
            "    test(\n",
            "  File \"/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/run.py\", line 84, in test\n",
            "    loss = criterion(outputs, labels)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3059, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "RuntimeError: expected scalar type Long but found Float\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定位到指定路径下\n",
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/SeptemberResult/result_DNN_CCE_epoch250')\n",
        "os.chdir('/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng')\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "ls5ZDn3v2AAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FEDRT-LSTM\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# time    : 2023/9/15 00:32\n",
        "# Author  : Liang-liang Zuo\n",
        "# @FileName: FEDRT-LSTM.py  DeepLearningBK-ZuoLiangliang/result_MultiAttention_LSTM_1层Transformer_MSE_epoch200_batchsize32_lr0.001_test0.1_95.54%.ipynb\n",
        "# @Software: Colab_Notebooks\n",
        "# 链接地址：https://github.com/zuo-liangliang/DeepLearningBK-ZuoLiangliang/blob/main/result_MultiAttention_LSTM_1%E5%B1%82Transformer_MSE_epoch200_batchsize32_lr0.001_test0.1_95.54%25.ipynb\n",
        "\n",
        "# 下载文件夹  计划下载的文件夹压成压缩包\n",
        "# import shutil\n",
        "\n",
        "# source_folder = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_Transformer_MultiAttention_LSTM_改costfunction为mse_1层Transformer_epoch200'  # 要压缩的文件夹路径\n",
        "# destination_zip = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/zip/result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_Transformer_MultiAttention_LSTM_改costfunction为mse_1层Transformer_epoch200'  # 压缩包保存路径\n",
        "\n",
        "# shutil.make_archive(destination_zip, 'zip', source_folder)\n",
        "\n",
        "# 定位到指定路径下\n",
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/SeptemberResult/result_MSE_epoch200_lr0.001_batchsize32所有网络汇总')\n",
        "# print(os.getcwd())\n",
        "\n",
        "# /content/drive/MyDrive/SeptemberResult/result_MSE_epoch200_lr0.001_batchsize32所有网络汇总\n",
        "\n",
        "# result_MultiAttention_LSTM_1层Transformer\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as scio\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.keras.backend.set_session\n",
        "import keras\n",
        "from keras.layers import Input, GRU, LSTM, Dense, Flatten, Dropout, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, TimeDistributed\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization  #加Transformer修改点1\n",
        "num_transformer_layers = 1  # 设置Transformer层数 #加Transformer修改点2  2层-->1层\n",
        "\n",
        "# Parameters\n",
        "# use_existing_model = False  # 创建一个模型（False）默认创建一个新模型\n",
        "use_existing_model = True    # 模型权重修改点 使用已经存在的模型（True）\n",
        "# data_dir = '/content/drive/MyDrive/Colab_Notebooks/528BVP-acnt2/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/'   #修改数据集文件夹\n",
        "\n",
        "data_dir_training = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/'\n",
        "data_dir_testing = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/'  # 修改为另外环境下的数据路径\n",
        "\n",
        "#fraction_for_test = 0.1 # 修改测试集占比 原先是0.1-->0.2\n",
        "#ALL_MOTION = [1,2,3,4,5,6]\n",
        "ALL_MOTION = [2,3,4,5]\n",
        "N_MOTION = len(ALL_MOTION)\n",
        "T_MAX = 0\n",
        "n_epochs = 100       #原始是30 #修改点1 取在528上最好的一组超参\n",
        "f_dropout_ratio = 0.5\n",
        "n_gru_hidden_units = 32\n",
        "n_batch_size = 32     #原始是32-->16\n",
        "f_learning_rate = 0.0001  #原始是0.001-->0.002\n",
        "\n",
        "\n",
        "def normalize_data(data_1):\n",
        "    # data(ndarray)=>data_norm(ndarray): [20,20,T]=>[20,20,T]\n",
        "    data_1_max = np.concatenate((data_1.max(axis=0),data_1.max(axis=1)),axis=0).max(axis=0)\n",
        "    data_1_min = np.concatenate((data_1.min(axis=0),data_1.min(axis=1)),axis=0).min(axis=0)\n",
        "    if (len(np.where((data_1_max - data_1_min) == 0)[0]) > 0):\n",
        "        return data_1\n",
        "    data_1_max_rep = np.tile(data_1_max,(data_1.shape[0],data_1.shape[1],1))\n",
        "    data_1_min_rep = np.tile(data_1_min,(data_1.shape[0],data_1.shape[1],1))\n",
        "    data_1_norm = (data_1 - data_1_min_rep) / (data_1_max_rep - data_1_min_rep)\n",
        "    return  data_1_norm\n",
        "\n",
        "def zero_padding(data, T_MAX):\n",
        "    # data(list)=>data_pad(ndarray): [20,20,T1/T2/...]=>[20,20,T_MAX]\n",
        "    data_pad = []\n",
        "    for i in range(len(data)):\n",
        "        t = np.array(data[i]).shape[2]\n",
        "        data_pad.append(np.pad(data[i], ((0,0),(0,0),(T_MAX - t,0)), 'constant', constant_values = 0).tolist())\n",
        "    return np.array(data_pad)\n",
        "\n",
        "def onehot_encoding(label, num_class):\n",
        "    # label(list)=>_label(ndarray): [N,]=>[N,num_class]\n",
        "    label = np.array(label).astype('int32')\n",
        "    # assert (np.arange(0,np.unique(label).size)==np.unique(label)).prod()    # Check label from 0 to N\n",
        "    label = np.squeeze(label)\n",
        "    # _label = np.eye(num_class)[label-1]     # from label to onehot\n",
        "    _label = np.eye(num_class)[label-2]   #修改点2\n",
        "    return _label\n",
        "\n",
        "def load_data(path_to_data, motion_sel):\n",
        "    global T_MAX\n",
        "    data = []\n",
        "    label = []\n",
        "    for data_root, data_dirs, data_files in os.walk(path_to_data):\n",
        "        for data_file_name in data_files:\n",
        "\n",
        "            file_path = os.path.join(data_root,data_file_name)\n",
        "            try:\n",
        "                data_1 = scio.loadmat(file_path)['velocity_spectrum_ro']\n",
        "                label_1 = int(data_file_name.split('-')[1])\n",
        "                #location = int(data_file_name.split('-')[2])\n",
        "                #orientation = int(data_file_name.split('-')[3])\n",
        "                #repetition = int(data_file_name.split('-')[4])\n",
        "                repetition = int(data_file_name.split('-')[2])  #修改点3\n",
        "\n",
        "                # Select Motion\n",
        "                if (label_1 not in motion_sel):\n",
        "                    continue\n",
        "\n",
        "                # Select Location\n",
        "                # if (location not in [1,2,3,5]):\n",
        "                #     continue\n",
        "\n",
        "                # Select Orientation\n",
        "                # if (orientation not in [1,2,4,5]):\n",
        "                #     continue\n",
        "\n",
        "                # Normalization\n",
        "                data_normed_1 = normalize_data(data_1)\n",
        "\n",
        "                # Update T_MAX\n",
        "                if T_MAX < np.array(data_1).shape[2]:\n",
        "                    T_MAX = np.array(data_1).shape[2]\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # Save List\n",
        "            data.append(data_normed_1.tolist())\n",
        "            label.append(label_1)\n",
        "                # Zero-padding\n",
        "    data = zero_padding(data, T_MAX)\n",
        "\n",
        "    # Swap axes\n",
        "    data = np.swapaxes(np.swapaxes(data, 1, 3), 2, 3)   # [N,20,20',T_MAX]=>[N,T_MAX,20,20']\n",
        "    data = np.expand_dims(data, axis=-1)    # [N,T_MAX,20,20]=>[N,T_MAX,20,20,1]\n",
        "\n",
        "    # Convert label to ndarray\n",
        "    label = np.array(label)\n",
        "\n",
        "    # data(ndarray): [N,T_MAX,20,20,1], label(ndarray): [N,N_MOTION]\n",
        "    return data, label\n",
        "\n",
        "# 构建符合输入形状为 (None, 20, 15, 15, 1) 和输出形状为 (None, 4) 的  模型：\n",
        "\n",
        "#加Transformer修改点3\n",
        "def transformer_layer(inputs, hidden_units, dropout_rate):\n",
        "    # Self-attention\n",
        "    attention_output = MultiHeadAttention(\n",
        "        num_heads=8, key_dim=hidden_units // 8\n",
        "    )(inputs, inputs)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = LayerNormalization()(attention_output + inputs)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(hidden_units, activation=\"relu\")(attention_output)\n",
        "    ffn = Dense(hidden_units)(ffn)\n",
        "    ffn = Dropout(dropout_rate)(ffn)\n",
        "    ffn = LayerNormalization()(ffn)\n",
        "\n",
        "    return ffn\n",
        "\n",
        "#加Transformer修改点4\n",
        "def assemble_model(input_shape, n_class):\n",
        "    model_input = Input(shape=input_shape, dtype='float32', name='name_model_input')    # (@,T_MAX,20,20,1)\n",
        "\n",
        "    # Feature extraction part\n",
        "    x = TimeDistributed(Conv2D(16,kernel_size=(5,5),activation='relu',data_format='channels_last',\\\n",
        "        input_shape=input_shape))(model_input)   # (@,T_MAX,20,20,1)=>(@,T_MAX,16,16,16)\n",
        "    x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)    # (@,T_MAX,16,16,16)=>(@,T_MAX,8,8,16)\n",
        "    x = TimeDistributed(Flatten())(x)   # (@,T_MAX,8,8,16)=>(@,T_MAX,8*8*16)\n",
        "    x = TimeDistributed(Dense(64,activation='relu'))(x) # (@,T_MAX,8*8*16)=>(@,T_MAX,64)\n",
        "    x = TimeDistributed(Dropout(f_dropout_ratio))(x)\n",
        "    x = TimeDistributed(Dense(64,activation='relu'))(x) # (@,T_MAX,64)=>(@,T_MAX,64)\n",
        "\n",
        "    # Add transformer layers\n",
        "    for _ in range(num_transformer_layers):\n",
        "        #x = transformer_layer(x, hidden_units=n_gru_hidden_units, dropout_rate=f_dropout_ratio)\n",
        "        x = transformer_layer(x, hidden_units=64, dropout_rate=f_dropout_ratio)  # 修改点1\n",
        "    x = LSTM(n_gru_hidden_units,return_sequences=False)(x)  # (@,T_MAX,64)=>(@,128)\n",
        "    x = Dropout(f_dropout_ratio)(x)\n",
        "    model_output = Dense(n_class, activation='softmax', name='name_model_output')(x)  # (@,128)=>(@,n_class)\n",
        "\n",
        "    # Create the model\n",
        "    # Model compiling\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(lr=f_learning_rate),\n",
        "                    # loss='categorical_crossentropy', # 将损失函数改为多类别交叉熵损失函数\n",
        "                    loss='mean_squared_error',  # 将损失函数改为均方误差\n",
        "                    # loss='mean_absolute_error',  # 将损失函数改为平均绝对误差\n",
        "                    # loss='binary_crossentropy',  # 将损失函数改为对数损失函数\n",
        "\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "    return model\n",
        "\n",
        "# 改\n",
        "#model_type = '双层LSTM'\n",
        "#model_type = 'LSTM'\n",
        "#model_type = 'BiLSTM'\n",
        "#model_type = 'MultiAttention_LSTM_1层Transformer'\n",
        "model_type = 'FEDRT-LSTM'\n",
        "\n",
        "# 创建结果文件夹\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_GRU_改costfunction为mse_epoch200' # 每次都要改\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_categorical_crossentropy_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_mean_squared_error_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_mean_absolute_error_epoch{n_epochs}'\n",
        "# result_folder = f'result_528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4_{model_type}_binary_crossentropy_epoch{n_epochs}'\n",
        "\n",
        "# result_folder = f'result_{model_type}_CCE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "# result_folder = f'result_{model_type}_MSE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "result_folder = f'Result_528TRAIN-zhangmengTEST'\n",
        "\n",
        "# result_folder = f'result_{model_type}_MAE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}'\n",
        "# result_folder = f'result_{model_type}_BCE_epoch{n_epochs}_batchsize{n_batch_size}_lr{f_learning_rate}_test{fraction_for_test}'\n",
        "\n",
        "os.makedirs(result_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "RcrUiRWM2gKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's BEGIN >>>>\n",
        "\n",
        "import time\n",
        "start_time = time.time()  # 记录程序开始时间\n",
        "\n",
        "# 获取可见的GPU设备列表\n",
        "# visible_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# 检查是否存在可见的GPU设备\n",
        "# if len(visible_gpus) < 1:\n",
        "    # print('No GPU available')\n",
        "    # exit(0)\n",
        "# 设置使用第一个可见的GPU设备\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(visible_gpus[0].index)\n",
        "# 配置GPU选项\n",
        "# gpu_config = tf.config.experimental.set_memory_growth(visible_gpus[0], True)\n",
        "# 创建一个会话并应用GPU配置\n",
        "# sess = tf.compat.v1.Session(config=gpu_config)\n",
        "# tf.compat.v1.keras.backend.set_session(sess)\n",
        "# tf.random.set_seed(1)\n",
        "\n",
        "# 获取可见的 GPU 设备列表\n",
        "visible_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(visible_gpus) > 0:\n",
        "    # 如果有可见的 GPU 设备，设置 TensorFlow 在 GPU 上运行\n",
        "    tf.config.set_visible_devices(visible_gpus, 'GPU')\n",
        "    gpu_config = tf.config.experimental.set_memory_growth(visible_gpus[0], True)\n",
        "    sess = tf.compat.v1.Session(config=gpu_config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "    print('GPU available')\n",
        "else:\n",
        "    # 如果没有可见的 GPU 设备，设置 TensorFlow 在 CPU 上运行\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "    sess = tf.compat.v1.Session()\n",
        "    print('No GPU available')\n",
        "\n",
        "# 加载训练集数据\n",
        "train_data, train_label = load_data(data_dir_training, ALL_MOTION)\n",
        "print('\\nLoaded dataset of ' + str(train_label.shape[0]) + ' samples, each sized ' + str(train_data[0,:,:].shape) + '\\n')\n",
        "\n",
        "##############################################################################\n",
        "# Load data\n",
        "# data, label = load_data(data_dir, ALL_MOTION)\n",
        "# print('\\nLoaded dataset of ' + str(label.shape[0]) + ' samples, each sized ' + str(data[0,:,:].shape) + '\\n')\n",
        "\n",
        "# Split train and test\n",
        "# [data_train, data_test, label_train, label_test] = train_test_split(data, label, test_size=fraction_for_test)\n",
        "# print('\\nTrain on ' + str(label_train.shape[0]) + ' samples\\n' +\\\n",
        "    # 'Test on ' + str(label_test.shape[0]) + ' samples\\n')\n",
        "\n",
        "# One-hot encoding for train data\n",
        "# label_train = onehot_encoding(label_train, N_MOTION)\n",
        "\n",
        "# Load or fabricate model\n",
        "if use_existing_model:\n",
        "    model = load_model('model_Transformer_MultiAttention_LSTM_mse_1层Transformer_epoch200_86.61%.h5')\n",
        "    # model = load_model('model_LSTM_91.07_best_trained.h5')\n",
        "    # model = load_model('model_MultiAttention_LSTM_1层Transformer_MSE_epoch200_BS32_lr0.001_86.61%.h5')\n",
        "    # model = load_model('MultiAttention_LSTM_mse_1层Transformer_epoch50_79.46%.h5')\n",
        "    model.summary()\n",
        "else:\n",
        "    #model = assemble_model(input_shape=(T_MAX, 20, 20, 1), n_class=N_MOTION)\n",
        "    model = assemble_model(input_shape=(T_MAX, 15, 15, 1), n_class=N_MOTION) #修改点4\n",
        "    model.summary()\n",
        "\n",
        "class AccuracyLossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.acc = []\n",
        "        self.loss = []\n",
        "        self.val_acc = []\n",
        "        self.val_loss = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.acc.append(logs.get('accuracy'))\n",
        "        self.loss.append(logs.get('loss'))\n",
        "        self.val_acc.append(logs.get('val_accuracy'))\n",
        "        self.val_loss.append(logs.get('val_loss'))\n",
        "\n",
        "history = AccuracyLossHistory()\n",
        "\n",
        "# model.fit({'name_model_input': data_train},{'name_model_output': label_train},\n",
        "model.fit({'name_model_input': train_data},{'name_model_output': train_label},\n",
        "          batch_size=n_batch_size,\n",
        "          epochs=n_epochs,\n",
        "          verbose=1,\n",
        "          # validation_split=0.1,\n",
        "          shuffle=True, callbacks=[history])\n",
        "print('Saving trained model...')\n",
        "#model.save('model_widar3_trained.h5')\n",
        "model.save(f'{result_folder}/model.h5')  # 改动\n",
        "\n",
        "# Testing...\n",
        "print('Testing...')\n",
        "# 加载测试集数据\n",
        "test_data, test_label = load_data(data_dir_testing, ALL_MOTION)\n",
        "\n",
        "# 在评估模型性能时，使用测试集的数据进行评估\n",
        "loss, accuracy = model.evaluate(test_data, test_label)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# label_test_pred = model.predict(data_test)\n",
        "# label_test_pred = np.argmax(label_test_pred, axis = -1) + 2 #修改点5\n",
        "\n",
        "# Confusion Matrix\n",
        "# cm = confusion_matrix(label_test, label_test_pred)\n",
        "# print(cm)\n",
        "# cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
        "# cm = np.around(cm, decimals=2) # 保留几位小数的意思\n",
        "# print(cm)\n",
        "#cm_display = ConfusionMatrixDisplay(cm, display_labels=range(1, N_MOTION+1)).plot()\n",
        "#cm_display = ConfusionMatrixDisplay(cm, display_labels=range(4, N_MOTION+3)).plot() #2345 lebel-2  4567 4=2+2  7=长度4+3\n",
        "# cm_display = ConfusionMatrixDisplay(cm, display_labels=range(2, N_MOTION+2)).plot() #2345 lebel-2  4567 4=2+2  7=长度4+3\n",
        "# plt.show()\n",
        "# plt.savefig(f'{result_folder}/cm.png') # 新增点1 保存混淆矩阵结果2023.09.11\n",
        "\n",
        "# ########################################################\n",
        "# 添加其他指标\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "\n",
        "# 假设label_test_pred是模型的预测结果，label_test是真实的标签\n",
        "# classification_result = classification_report(label_test, label_test_pred, digits=4)\n",
        "\n",
        "# print(classification_result)\n",
        "\n",
        "# 将分类报告保存到文本文件\n",
        "# classification_file = os.path.join(result_folder, 'classification_report.txt')\n",
        "# with open(classification_file, 'w') as f:\n",
        "    # f.write(classification_result)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "# Accuracy 新增百分号\n",
        "# test_accuracy = np.sum(label_test == label_test_pred) / (label_test.shape[0])\n",
        "#print(test_accuracy)\n",
        "# test_accuracy_percentage = test_accuracy * 100\n",
        "# test_accuracy_str = f\"{test_accuracy_percentage:.2f}%\"\n",
        "# print(test_accuracy_str)\n",
        "\n",
        "# df_accuracy = pd.DataFrame({'Test Accuracy': [test_accuracy]})\n",
        "# df_accuracy.to_excel(f'{result_folder}/{test_accuracy_str}.xlsx', index=False) ## 新增点2 保存测试集平均准确率结果2023.09.11\n",
        "\n",
        "# df = pd.DataFrame({\n",
        "    # 'Epoch': range(1, n_epochs+1),\n",
        "    # 'Train Accuracy': history.acc,\n",
        "    # 'Train Loss': history.loss,\n",
        "    # 'Validation Accuracy': history.val_acc,\n",
        "    # 'Validation Loss': history.val_loss\n",
        "# })\n",
        "\n",
        "# df.to_excel('accuracy_l'''  '''oss.xlsx', index=False)\n",
        "# 保存准确率和损失数据到Excel表格中\n",
        "# df.to_excel(f'{result_folder}/accuracy_loss.xlsx', index=False) #改动\n",
        "\n",
        "# plt.plot(range(1, n_epochs+1), history.acc, label='Train Accuracy')\n",
        "# plt.plot(range(1, n_epochs+1), history.loss, label='Train Loss')\n",
        "# plt.plot(range(1, n_epochs+1), history.val_acc, label='Validation Accuracy')\n",
        "# plt.plot(range(1, n_epochs+1), history.val_loss, label='Validation Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend()\n",
        "# plt.savefig(f'{result_folder}/plot.png')\n",
        "# plt.show()\n",
        "\n",
        "# 新增 修改文件名 并打印新的文件名\n",
        "# new_result_folder = result_folder + f'_{test_accuracy_str}'\n",
        "# os.rename(result_folder, new_result_folder)\n",
        "# result_folder = new_result_folder\n",
        "\n",
        "# print(\"新的结果文件夹名称为：\", result_folder)\n",
        "\n",
        "end_time = time.time()  # 记录程序结束时间\n",
        "duration = end_time - start_time  # 计算程序运行时间\n",
        "print(\"程序执行时间为：{:.2f}秒\".format(duration))\n",
        "\n",
        "with open(f'{result_folder}/runtime.txt', 'w') as f:\n",
        "    f.write(\"程序执行时间为：{:.2f}秒\".format(duration)) # 新增点3 保存程序执行时间结果2023.09.11\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_ujmBdVPZzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 指定文件夹路径\n",
        "folder_path = \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4\"\n",
        "\n",
        "# 获取文件夹下所有子文件夹的名称\n",
        "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
        "\n",
        "# 遍历子文件夹，并重命名\n",
        "for subfolder in subfolders:\n",
        "    # 提取子文件夹的名称\n",
        "    folder_name = os.path.basename(subfolder)\n",
        "    # 构建新的文件夹名称，加上\"user\"前缀\n",
        "    new_folder_name = \"user\" + folder_name\n",
        "    # 构建新的文件夹路径\n",
        "    new_folder_path = os.path.join(folder_path, new_folder_name)\n",
        "    # 重命名文件夹\n",
        "    os.rename(subfolder, new_folder_path)\n",
        "    print(f\"已将文件夹 {folder_name} 重命名为 {new_folder_name}\")\n",
        "\n",
        "print(\"所有子文件夹重命名完成！\")\n"
      ],
      "metadata": {
        "id": "C1sJnUtJ9kw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4.zip\" -d \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/528[-1,1]长度20-分辨率15_acnt2_剔14_改6为4/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "ubPGdKV27BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/DriverBVP[-1,1]bvplength20-分辨率15-手势4.zip\" -d \"/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/\"\n",
        "# 解压Zip文件到指定路径\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "Jak9iQPv8t6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 先userx-后面动作的5改为6，然后3改为5\n",
        "\n",
        "import os\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user2'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 解析文件名，提取a和b的值\n",
        "    parts = filename.split('-')\n",
        "    a = int(parts[1])\n",
        "    b = int(parts[2])\n",
        "\n",
        "    #str.split(“o”)[0]得到的是第一个o之前的内容\n",
        "    #str.split(“o”)[1]得到的是第一个o和第二个o之间的内容\n",
        "    #str.split(“o”)[3]得到的是第三个o后和第四个o前之间的内容\n",
        "    #str.split(\"[\")[0]得到的是第一个 [ 之前的内容\n",
        "\n",
        "    # 第一步：将a=5的文件的a值改为6   #####################两步分次执行！！！\n",
        "    if a == 5:\n",
        "        new_a = 6\n",
        "    else:\n",
        "        new_a = a\n",
        "\n",
        "    # 第二步：将a=3的文件的a值改为5   #####################两步分次执行！！！\n",
        "    # if a == 3:\n",
        "        # new_a = 5\n",
        "    # else:\n",
        "        # new_a = a\n",
        "\n",
        "    # 构造新文件名\n",
        "    new_filename = f\"user2-{new_a}-{b}-1.mat\"\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "ufeaBfsM_jEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK\n",
        "import os\n",
        "################################################################################################################################\n",
        "# 执行复制操作\n",
        "!cp -r /content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4-BK/user1 /content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1\n",
        "\n",
        "# 检查目标文件夹是否存在\n",
        "target_folder = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1' # 此时的张梦数据更改了文件夹和文件前缀，加了user\n",
        "if os.path.exists(target_folder):\n",
        "    print(\"复制成功！\")\n",
        "else:\n",
        "    print(\"复制失败，请检查路径是否正确。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEg9OoKi_Bvz",
        "outputId": "cc940f22-7193-4d8a-b8d8-b5423faab657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "复制成功！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user14'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 构造新文件名，加上\"user\"前缀\n",
        "    new_filename = 'user' + filename\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "6EsIyEUD-TLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 指定要修改文件名的路径\n",
        "directory = '/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/DriverBVP[-1,1]bvplength20-分辨率15-手势4/user1'\n",
        "\n",
        "# 遍历指定路径下的所有文件\n",
        "for filename in os.listdir(directory):\n",
        "    # 构造原始文件的完整路径\n",
        "    old_path = os.path.join(directory, filename)\n",
        "\n",
        "    # 去掉文件名中的所有下划线\"_\"\n",
        "    new_filename = filename.replace('_', '')\n",
        "\n",
        "    # 构造新文件的完整路径\n",
        "    new_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    # 重命名文件\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"已将文件 {filename} 重命名为 {new_filename}\")\n"
      ],
      "metadata": {
        "id": "AK5ePxoi-kpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "# 删除多个文件夹\n",
        "import shutil\n",
        "# 要删除的文件夹路径列表\n",
        "folder_paths = [\n",
        "    #'/content/drive/My Drive/example_folder1',\n",
        "    #'/content/drive/My Drive/example_folder2',\n",
        "    #'/content/drive/My Drive/example_folder3',\n",
        "    # 添加更多的文件夹路径...\n",
        "    # test\n",
        "    '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi_HAR',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user2',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user3',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user4',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user5',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user6',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user7',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user8',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user9',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user10',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user11',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user12',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user13',\n",
        "    #'/content/drive/MyDrive/Result_FEDRT-LSTM_TSET-use-zhangmeng/user14',\n",
        "]\n",
        "# 遍历文件夹路径列表，逐个删除文件夹\n",
        "for folder_path in folder_paths:\n",
        "    try:\n",
        "        shutil.rmtree(folder_path)\n",
        "        print(f\"文件夹 '{folder_path}' 删除成功！\")\n",
        "    except Exception as e:\n",
        "        print(f\"删除文件夹 '{folder_path}' 时出现错误：{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpUpMZhk776E",
        "outputId": "537388e3-88a4-49b0-8c09-2988773d2394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件夹 '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/NTU-Fi_HAR' 删除成功！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_INGrZ9z6iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "################################################################################################################################\n",
        "# 执行复制操作\n",
        "!cp -r /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata /content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK\n",
        "\n",
        "# 检查目标文件夹是否存在\n",
        "target_folder = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK'\n",
        "if os.path.exists(target_folder):\n",
        "    print(\"复制成功！\")\n",
        "else:\n",
        "    print(\"复制失败，请检查路径是否正确。\")\n"
      ],
      "metadata": {
        "id": "UYJ_yWJUyt5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################\n",
        "import shutil\n",
        "\n",
        "# 要删除的文件夹路径\n",
        "folder_to_delete = '/content/drive/MyDrive/check_yjf_2024.04.10/WiFi-CSI-Sensing-Benchmark-main/Widardata-BK'\n",
        "\n",
        "# 使用 shutil.rmtree 递归删除文件夹及其内容\n",
        "shutil.rmtree(folder_to_delete)\n"
      ],
      "metadata": {
        "id": "vUUV9XGzx9HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}